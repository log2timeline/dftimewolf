{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dfTimewolf \u00b6 A framework for orchestrating forensic collection, processing and data export. dfTimewolf consists of collectors, processors and exporters (modules) that pass data on to one another. How modules are orchestrated is defined in predefined \"recipes\".","title":"Home"},{"location":"#dftimewolf","text":"A framework for orchestrating forensic collection, processing and data export. dfTimewolf consists of collectors, processors and exporters (modules) that pass data on to one another. How modules are orchestrated is defined in predefined \"recipes\".","title":"dfTimewolf"},{"location":"architecture/","text":"Architecture \u00b6 The main concepts you need to be aware of when digging into dfTimewolf's codebase are: Modules Recipes The state object Modules are individual Python objects that will interact with specific platforms depending on attributes passed through the command line or AttributeContainer objects created by a previous module's execution. Recipes are instructions that define how modules are chained, essentially defining which Module's output becomes another Module's input. Input and output are all stored in a State object that is attached to each module. Modules \u00b6 Modules all extend the BaseModule class , and implement the SetUp , and Process functions. SetUp is what is called with the recipe's modified arguments. Actions here should include things that have low overhead and can be accomplished with no big delay, like checking for API permissions, verifying that a file exists, etc. The idea here is to detect working conditions and \"fail early\" if the module can't run correctly. Process is where all the magic happens - here is where you'll want to parallelize (see also Thread Aware Modules ) things as much as possible (copying a disk, running plaso, etc.). You'll be reading from containers pushed by previous modules (e.g. processed plaso files) and adding your own for future modules to process. Accessing containers is done through the GetContainers and StoreContainer functions of the state object. Tip: If you want your module to be able to take inputs from both recipe arguments or the state, consider including something like the following in your SetUp : for p in param.split(','): self.StoreContainer(containers.MyContainer(p)) This way, any recipe arguments (in this example, comma separated) are available in Process via self.GetContainers() , in addition to any containers from previous modules. Thread Aware Modules \u00b6 If your module takes multiple inputs you can take advantage of the ThreadAwareModule base class to have your inputs processed in parallel threads. The following are the differences from implementing BaseModule : Process takes a single container argument. You process this single container, rather than sourcing containers to process from self.GetContainers() . Required method overrides: GetThreadOnContainerType() - The type of container that is to be used as input to the parallel threads. GetThreadPoolSize() - Determine the maximum number of simultaneous threads. Optional method overrides: PreProcess() & PostProcess() - Work that needs to be done prior to, or after Process(container) , that only occurs once regardless of the number of inputs. KeepThreadedContainersInState() - Used to determine whether the containers passed to Process(container) should be removed from the state after processing. Logging \u00b6 Modules can log messages to make the execution flow clearer for the user. This is done through the module's logger attribute: self.logger.info('message') . This uses the standard python logging module so can use functions like info , warning , debug . Error reporting \u00b6 Modules can also report errors using their ModuleError function. Errors added this way will be reported at the end of the run. Semantically, they mean that the recipe flow didn't go as expected and should be examined. ModuleError also takes a critical parameter, which will raise an exception and interrupt the flow of the whole recipe. This should be used for errors that dftimewolf can't recover from (e.g. if a binary run by one of the modules can't be found on disk). Recipes \u00b6 Recipes are JSON files that describe how Modules are chained, and which parameters can be ingested from the command-line. A recipe JSON object follows a specific format: name : This is the name with which the recipe will be invoked (e.g. plaso_ts ). description : This is a longer description of what the recipe does. It will show up in the help message when invoking dftimewolf recipe_hame -h . short_description : This is what will show up in the help message when invoking dftimewolf -h . modules : An array of JSON objects describing modules and their corresponding arguments. wants : What other modules this module should wait for before calling its Process function. name : The name of the module class that will be instantiated. runtime_name : Optional argument, use this for recipes when you're using the same module more than once. args : A list of (argument_name, argument) tuples that will be passed on to the module's SetUp() function. If argument starts with an @ , it will be replaced with its corresponding value from the command-line or the ~/.dftimewolfrc file. args : Recipes need to describe the way arguments are handled in a global args variable. This variable is a list of (switch, help_message, default_value) tuples that will be passed to the argparse.add_argument function for later parsing. State and AttributeContainers \u00b6 The State object is an instance of the DFTimewolfState class . It has a couple of useful functions and attributes: StoreContainer : Store your containers to make them available to future modules. GetContainers : Retrieve the containers stored using StoreContainer . It takes a container_class param where you can select which containers you're interested in. RegisterStreamingCallback : Use this to register a function that will be called on the container as it is streamed in real-time. Life of a dfTimewolf run \u00b6 The dfTimewolf cycle is as follows: The recipe JSON is parsed, all requested modules are instantiated, as well as the semaphores that will schedule the execution of the Module's Process functions. Command-line arguments are taken into account and passed to Module's SetUp function. This occurs in parallel for all modules, regardless of the semaphores they declared in the recipe. The modules with no blocking semaphores start running their Process function. At the end of their run, they free their semaphore, signalling other modules that they can proceed with their own Process function. This cycle repeats until all modules have called their Process function.","title":"Architecture"},{"location":"architecture/#architecture","text":"The main concepts you need to be aware of when digging into dfTimewolf's codebase are: Modules Recipes The state object Modules are individual Python objects that will interact with specific platforms depending on attributes passed through the command line or AttributeContainer objects created by a previous module's execution. Recipes are instructions that define how modules are chained, essentially defining which Module's output becomes another Module's input. Input and output are all stored in a State object that is attached to each module.","title":"Architecture"},{"location":"architecture/#modules","text":"Modules all extend the BaseModule class , and implement the SetUp , and Process functions. SetUp is what is called with the recipe's modified arguments. Actions here should include things that have low overhead and can be accomplished with no big delay, like checking for API permissions, verifying that a file exists, etc. The idea here is to detect working conditions and \"fail early\" if the module can't run correctly. Process is where all the magic happens - here is where you'll want to parallelize (see also Thread Aware Modules ) things as much as possible (copying a disk, running plaso, etc.). You'll be reading from containers pushed by previous modules (e.g. processed plaso files) and adding your own for future modules to process. Accessing containers is done through the GetContainers and StoreContainer functions of the state object. Tip: If you want your module to be able to take inputs from both recipe arguments or the state, consider including something like the following in your SetUp : for p in param.split(','): self.StoreContainer(containers.MyContainer(p)) This way, any recipe arguments (in this example, comma separated) are available in Process via self.GetContainers() , in addition to any containers from previous modules.","title":"Modules"},{"location":"architecture/#thread-aware-modules","text":"If your module takes multiple inputs you can take advantage of the ThreadAwareModule base class to have your inputs processed in parallel threads. The following are the differences from implementing BaseModule : Process takes a single container argument. You process this single container, rather than sourcing containers to process from self.GetContainers() . Required method overrides: GetThreadOnContainerType() - The type of container that is to be used as input to the parallel threads. GetThreadPoolSize() - Determine the maximum number of simultaneous threads. Optional method overrides: PreProcess() & PostProcess() - Work that needs to be done prior to, or after Process(container) , that only occurs once regardless of the number of inputs. KeepThreadedContainersInState() - Used to determine whether the containers passed to Process(container) should be removed from the state after processing.","title":"Thread Aware Modules"},{"location":"architecture/#logging","text":"Modules can log messages to make the execution flow clearer for the user. This is done through the module's logger attribute: self.logger.info('message') . This uses the standard python logging module so can use functions like info , warning , debug .","title":"Logging"},{"location":"architecture/#error-reporting","text":"Modules can also report errors using their ModuleError function. Errors added this way will be reported at the end of the run. Semantically, they mean that the recipe flow didn't go as expected and should be examined. ModuleError also takes a critical parameter, which will raise an exception and interrupt the flow of the whole recipe. This should be used for errors that dftimewolf can't recover from (e.g. if a binary run by one of the modules can't be found on disk).","title":"Error reporting"},{"location":"architecture/#recipes","text":"Recipes are JSON files that describe how Modules are chained, and which parameters can be ingested from the command-line. A recipe JSON object follows a specific format: name : This is the name with which the recipe will be invoked (e.g. plaso_ts ). description : This is a longer description of what the recipe does. It will show up in the help message when invoking dftimewolf recipe_hame -h . short_description : This is what will show up in the help message when invoking dftimewolf -h . modules : An array of JSON objects describing modules and their corresponding arguments. wants : What other modules this module should wait for before calling its Process function. name : The name of the module class that will be instantiated. runtime_name : Optional argument, use this for recipes when you're using the same module more than once. args : A list of (argument_name, argument) tuples that will be passed on to the module's SetUp() function. If argument starts with an @ , it will be replaced with its corresponding value from the command-line or the ~/.dftimewolfrc file. args : Recipes need to describe the way arguments are handled in a global args variable. This variable is a list of (switch, help_message, default_value) tuples that will be passed to the argparse.add_argument function for later parsing.","title":"Recipes"},{"location":"architecture/#state-and-attributecontainers","text":"The State object is an instance of the DFTimewolfState class . It has a couple of useful functions and attributes: StoreContainer : Store your containers to make them available to future modules. GetContainers : Retrieve the containers stored using StoreContainer . It takes a container_class param where you can select which containers you're interested in. RegisterStreamingCallback : Use this to register a function that will be called on the container as it is streamed in real-time.","title":"State and AttributeContainers"},{"location":"architecture/#life-of-a-dftimewolf-run","text":"The dfTimewolf cycle is as follows: The recipe JSON is parsed, all requested modules are instantiated, as well as the semaphores that will schedule the execution of the Module's Process functions. Command-line arguments are taken into account and passed to Module's SetUp function. This occurs in parallel for all modules, regardless of the semaphores they declared in the recipe. The modules with no blocking semaphores start running their Process function. At the end of their run, they free their semaphore, signalling other modules that they can proceed with their own Process function. This cycle repeats until all modules have called their Process function.","title":"Life of a dfTimewolf run"},{"location":"developers-guide/","text":"Developer's guide \u00b6 This page gives a few hints on how to develop new recipes and modules for dftimewolf. Start with the architecture page if you haven't read it already. Environment setup \u00b6 You can either use Poetry or Docker to develop on dftimewolf. Python virtualenv with Poetry \u00b6 To be able to develop you need a local installation of dfTimewolf. To install it locally, a venv is recommended. We use poetry: pip install poetry poetry install Now you are ready to run dfTimewolf in your local environment: poetry run dftimewolf -h Docker container \u00b6 We also provide a dev Docker container that you can use to install dftimewolf's dependencies in. $ cd docker/dev $ docker compose run --rm dftw envshell [...] (dftimewolf-py3.10) root@3bb3a1ddec53:/app# poetry run dftimewolf -h It will pick up changes from your current working directory, so tests will run with the version of the code present on your filesystem. See docker/dev/README.md for more details. Code review & code style \u00b6 As for other Log2Timeline projects, all contributions to dfTimewolf undergo code review. The process is documented here . dfTimewolf follows the Log2Timeline style guide . Creating a recipe \u00b6 If you're not satisfied with the way modules are chained, or default arguments that are passed to some of the recipes, then you can create your own. See existing recipes for simple examples like plaso_ts . Details on recipe keys are given here . Recipe location \u00b6 A new recipe needs to be added to data/recipes as a JSON file. Recipe arguments \u00b6 Recipes launch Modules with a given set of arguments. Arguments can be specified in different ways: Hardcoded values in the recipe's Python code @ parameters that are dynamically changed, either: Through a ~/.dftimewolfrc file Through the command line Parameters for each Module are declared in a recipe's recipe key in the form of @parameter placeholders. How these are populated is then specified in the args key right after, as a list of (argument, help_text, default_value) tuples that will be passed to argparse . For example, the public version of the grr_artifact_hosts.py recipe specifies arguments in the following way: \"args\": [ [\"source_project_name\", \"Name of the project containing the instance / disks to copy.\", null], [\"analysis_project_name\", \"Name of the project where the analysis VM will be created and disks copied to.\", null], [\"--incident_id\", \"Incident ID to label the VM with.\", null], [\"--instances\", \"Name of the instance to analyze.\", null], [\"--disks\", \"Comma-separated list of disks to copy from the source GCP project (if `instance` not provided).\", null], [\"--all_disks\", \"Copy all disks in the designated instance. Overrides `disk_names` if specified.\", false], [\"--stop_instances\", \"Stop the designated instance after copying disks.\", false], [\"--create_analysis_vm\", \"Create an analysis VM in the destination project.\", true], [\"--cpu_cores\", \"Number of CPU cores of the analysis VM.\", 4], [\"--boot_disk_size\", \"The size of the analysis VM boot disk (in GB).\", 50.0], [\"--boot_disk_type\", \"Disk type to use [pd-standard, pd-ssd].\", \"pd-standard\"], [\"--zone\", \"The GCP zone where the Analysis VM and copied disks will be created.\", \"us-central1-f\"] ] source_project_name and analysis_project_name are positional arguments - they must be provided through the command line. incident_id , instance , disks , all_disks , and all other arguments starting with -- are optional. If they are not specified through the command line, the default value will be used. null will be translated to a Python None , and false will be the python False boolean. Note It's currnetly not possible to make a parameter `false` through a flag. We recommend adding parameters that default to `false`, and turn `true` when provided in the command line. Modules \u00b6 If dftimewolf lacks the actual processing logic, you need to create a new module. If you can achieve your goal in Python, then chances are you can write a dfTimewolf module for it. Check out the Module architecture and read up on simple existing modules such as the LocalPlasoProcessor module for an example of simple Module. Additionally, a guide on implementing a module exists here Register a new module \u00b6 There are two locations to register new modules: registering it at the end of the module file adding it to the MODULES dictionary in the main entry point script dftimewolf_recipes.py Run tests \u00b6 It is recommended to run tests locally to discover issues early in the development lifecycle. pip install pylint pylint --rcfile .pylintrc UPDATED_SOURCE_FILE pip install poetry poetry install poetry run python -m unittest discover -s tests -p '*.py' poetry run mypy --install-types --cache-dir .mypy_cache --non-interactive -p dftimewolf poetry run pytype --config pytype.conf Docker equivalent: cd docker/dev docker compose run --rm dftw tests docker compose run --rm -v `pwd`/../../:/app dftw poetry run mypy --install-types --cache-dir /tmp/.mypy_cache --non-interactive -p dftimewolf docker compose run --rm -v `pwd`/../../:/app dftw poetry run pytype --config /app/pytype.conf","title":"Developers' guide"},{"location":"developers-guide/#developers-guide","text":"This page gives a few hints on how to develop new recipes and modules for dftimewolf. Start with the architecture page if you haven't read it already.","title":"Developer's guide"},{"location":"developers-guide/#environment-setup","text":"You can either use Poetry or Docker to develop on dftimewolf.","title":"Environment setup"},{"location":"developers-guide/#python-virtualenv-with-poetry","text":"To be able to develop you need a local installation of dfTimewolf. To install it locally, a venv is recommended. We use poetry: pip install poetry poetry install Now you are ready to run dfTimewolf in your local environment: poetry run dftimewolf -h","title":"Python virtualenv with Poetry"},{"location":"developers-guide/#docker-container","text":"We also provide a dev Docker container that you can use to install dftimewolf's dependencies in. $ cd docker/dev $ docker compose run --rm dftw envshell [...] (dftimewolf-py3.10) root@3bb3a1ddec53:/app# poetry run dftimewolf -h It will pick up changes from your current working directory, so tests will run with the version of the code present on your filesystem. See docker/dev/README.md for more details.","title":"Docker container"},{"location":"developers-guide/#code-review-code-style","text":"As for other Log2Timeline projects, all contributions to dfTimewolf undergo code review. The process is documented here . dfTimewolf follows the Log2Timeline style guide .","title":"Code review &amp; code style"},{"location":"developers-guide/#creating-a-recipe","text":"If you're not satisfied with the way modules are chained, or default arguments that are passed to some of the recipes, then you can create your own. See existing recipes for simple examples like plaso_ts . Details on recipe keys are given here .","title":"Creating a recipe"},{"location":"developers-guide/#recipe-location","text":"A new recipe needs to be added to data/recipes as a JSON file.","title":"Recipe location"},{"location":"developers-guide/#recipe-arguments","text":"Recipes launch Modules with a given set of arguments. Arguments can be specified in different ways: Hardcoded values in the recipe's Python code @ parameters that are dynamically changed, either: Through a ~/.dftimewolfrc file Through the command line Parameters for each Module are declared in a recipe's recipe key in the form of @parameter placeholders. How these are populated is then specified in the args key right after, as a list of (argument, help_text, default_value) tuples that will be passed to argparse . For example, the public version of the grr_artifact_hosts.py recipe specifies arguments in the following way: \"args\": [ [\"source_project_name\", \"Name of the project containing the instance / disks to copy.\", null], [\"analysis_project_name\", \"Name of the project where the analysis VM will be created and disks copied to.\", null], [\"--incident_id\", \"Incident ID to label the VM with.\", null], [\"--instances\", \"Name of the instance to analyze.\", null], [\"--disks\", \"Comma-separated list of disks to copy from the source GCP project (if `instance` not provided).\", null], [\"--all_disks\", \"Copy all disks in the designated instance. Overrides `disk_names` if specified.\", false], [\"--stop_instances\", \"Stop the designated instance after copying disks.\", false], [\"--create_analysis_vm\", \"Create an analysis VM in the destination project.\", true], [\"--cpu_cores\", \"Number of CPU cores of the analysis VM.\", 4], [\"--boot_disk_size\", \"The size of the analysis VM boot disk (in GB).\", 50.0], [\"--boot_disk_type\", \"Disk type to use [pd-standard, pd-ssd].\", \"pd-standard\"], [\"--zone\", \"The GCP zone where the Analysis VM and copied disks will be created.\", \"us-central1-f\"] ] source_project_name and analysis_project_name are positional arguments - they must be provided through the command line. incident_id , instance , disks , all_disks , and all other arguments starting with -- are optional. If they are not specified through the command line, the default value will be used. null will be translated to a Python None , and false will be the python False boolean. Note It's currnetly not possible to make a parameter `false` through a flag. We recommend adding parameters that default to `false`, and turn `true` when provided in the command line.","title":"Recipe arguments"},{"location":"developers-guide/#modules","text":"If dftimewolf lacks the actual processing logic, you need to create a new module. If you can achieve your goal in Python, then chances are you can write a dfTimewolf module for it. Check out the Module architecture and read up on simple existing modules such as the LocalPlasoProcessor module for an example of simple Module. Additionally, a guide on implementing a module exists here","title":"Modules"},{"location":"developers-guide/#register-a-new-module","text":"There are two locations to register new modules: registering it at the end of the module file adding it to the MODULES dictionary in the main entry point script dftimewolf_recipes.py","title":"Register a new module"},{"location":"developers-guide/#run-tests","text":"It is recommended to run tests locally to discover issues early in the development lifecycle. pip install pylint pylint --rcfile .pylintrc UPDATED_SOURCE_FILE pip install poetry poetry install poetry run python -m unittest discover -s tests -p '*.py' poetry run mypy --install-types --cache-dir .mypy_cache --non-interactive -p dftimewolf poetry run pytype --config pytype.conf Docker equivalent: cd docker/dev docker compose run --rm dftw tests docker compose run --rm -v `pwd`/../../:/app dftw poetry run mypy --install-types --cache-dir /tmp/.mypy_cache --non-interactive -p dftimewolf docker compose run --rm -v `pwd`/../../:/app dftw poetry run pytype --config /app/pytype.conf","title":"Run tests"},{"location":"getting-started/","text":"Getting started \u00b6 Installation \u00b6 Ideally you'll want to install dftimewolf in its own virtual environment. git clone https://github.com/log2timeline/dftimewolf.git && cd dftimewolf pip install poetry poetry install Attention If you want to leverage other modules such as log2timeline, you'll have to install them separately and make them available in your virtual environment. If you want to run dftimewolf from any other directory, activate the virtualenv by doing poetry shell in the main dfTimewolf directory. Quick how-to \u00b6 dfTimewolf is typically run by specifying a recipe name and any arguments the recipe defines. For example: dftimewolf plaso_ts /tmp/path1,/tmp/path2 --incident_id 12345 This will launch the plaso_ts recipe against path1 and path2 in /tmp . In this recipe --incident_id is used by Timesketch as a sketch description. Details on a recipe can be obtained using the standard python help flags: $ dftimewolf -h [2020-10-06 14:29:42,111] [dftimewolf ] INFO Logging to stdout and /tmp/dftimewolf.log [2020-10-06 14:29:42,111] [dftimewolf ] DEBUG Recipe data path: /Users/tomchop/code/dftimewolf/data [2020-10-06 14:29:42,112] [dftimewolf ] DEBUG Configuration loaded from: /Users/tomchop/code/dftimewolf/data/config.json usage: dftimewolf [-h] {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,gcp_logging_cloudsql_ts,...} Available recipes: aws_forensics Copies a volume from an AWS account to an analysis VM. gce_disk_export Export disk image from a GCP project to Google Cloud Storage. gcp_forensics Copies disk from a GCP project to an analysis VM. gcp_logging_cloudaudit_ts Collects GCP logs from a project and exports them to Timesketch. [...] positional arguments: {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,...} optional arguments: -h, --help show this help message and exit To get details on an individual recipe, call the recipe with the -h flag. $ dftimewolf gcp_forensics -h [...] usage: dftimewolf gcp_forensics [-h] [--instance INSTANCE] [--disks DISKS] [--all_disks] [--analysis_project_name ANALYSIS_PROJECT_NAME] [--boot_disk_size BOOT_DISK_SIZE] [--boot_disk_type BOOT_DISK_TYPE] [--zone ZONE] remote_project_name incident_id Copies a disk from a project to another, creates an analysis VM, and attaches the copied disk to it. positional arguments: remote_project_name Name of the project containing the instance / disks to copy incident_id Incident ID to label the VM with. optional arguments: -h, --help show this help message and exit --instance INSTANCE Name of the instance to analyze. (default: None) --disks DISKS Comma-separated list of disks to copy. (default: None) --all_disks Copy all disks in the designated instance. Overrides disk_names if specified (default: False) --analysis_project_name ANALYSIS_PROJECT_NAME Name of the project where the analysis VM will be created (default: None) --boot_disk_size BOOT_DISK_SIZE The size of the analysis VM boot disk (in GB) (default: 50.0) --boot_disk_type BOOT_DISK_TYPE Disk type to use [pd-standard, pd-ssd] (default: pd- standard) --zone ZONE The GCP zone where the Analysis VM and copied disks will be created (default: us-central1-f)","title":"Getting started"},{"location":"getting-started/#getting-started","text":"","title":"Getting started"},{"location":"getting-started/#installation","text":"Ideally you'll want to install dftimewolf in its own virtual environment. git clone https://github.com/log2timeline/dftimewolf.git && cd dftimewolf pip install poetry poetry install Attention If you want to leverage other modules such as log2timeline, you'll have to install them separately and make them available in your virtual environment. If you want to run dftimewolf from any other directory, activate the virtualenv by doing poetry shell in the main dfTimewolf directory.","title":"Installation"},{"location":"getting-started/#quick-how-to","text":"dfTimewolf is typically run by specifying a recipe name and any arguments the recipe defines. For example: dftimewolf plaso_ts /tmp/path1,/tmp/path2 --incident_id 12345 This will launch the plaso_ts recipe against path1 and path2 in /tmp . In this recipe --incident_id is used by Timesketch as a sketch description. Details on a recipe can be obtained using the standard python help flags: $ dftimewolf -h [2020-10-06 14:29:42,111] [dftimewolf ] INFO Logging to stdout and /tmp/dftimewolf.log [2020-10-06 14:29:42,111] [dftimewolf ] DEBUG Recipe data path: /Users/tomchop/code/dftimewolf/data [2020-10-06 14:29:42,112] [dftimewolf ] DEBUG Configuration loaded from: /Users/tomchop/code/dftimewolf/data/config.json usage: dftimewolf [-h] {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,gcp_logging_cloudsql_ts,...} Available recipes: aws_forensics Copies a volume from an AWS account to an analysis VM. gce_disk_export Export disk image from a GCP project to Google Cloud Storage. gcp_forensics Copies disk from a GCP project to an analysis VM. gcp_logging_cloudaudit_ts Collects GCP logs from a project and exports them to Timesketch. [...] positional arguments: {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,...} optional arguments: -h, --help show this help message and exit To get details on an individual recipe, call the recipe with the -h flag. $ dftimewolf gcp_forensics -h [...] usage: dftimewolf gcp_forensics [-h] [--instance INSTANCE] [--disks DISKS] [--all_disks] [--analysis_project_name ANALYSIS_PROJECT_NAME] [--boot_disk_size BOOT_DISK_SIZE] [--boot_disk_type BOOT_DISK_TYPE] [--zone ZONE] remote_project_name incident_id Copies a disk from a project to another, creates an analysis VM, and attaches the copied disk to it. positional arguments: remote_project_name Name of the project containing the instance / disks to copy incident_id Incident ID to label the VM with. optional arguments: -h, --help show this help message and exit --instance INSTANCE Name of the instance to analyze. (default: None) --disks DISKS Comma-separated list of disks to copy. (default: None) --all_disks Copy all disks in the designated instance. Overrides disk_names if specified (default: False) --analysis_project_name ANALYSIS_PROJECT_NAME Name of the project where the analysis VM will be created (default: None) --boot_disk_size BOOT_DISK_SIZE The size of the analysis VM boot disk (in GB) (default: 50.0) --boot_disk_type BOOT_DISK_TYPE Disk type to use [pd-standard, pd-ssd] (default: pd- standard) --zone ZONE The GCP zone where the Analysis VM and copied disks will be created (default: us-central1-f)","title":"Quick how-to"},{"location":"module-writing-basics/","text":"How to write a DFTimewolf Module \u00b6 The purpose of this guide is to walk a contributor through the basics of how to write a DFTimewolf module. Our example will perform the following: Receive filepaths of files on the local filesystem Hash the files and check against a list of hashes Move matching files to an output directory Keep only matching files in the state for processing by a later module 0 - An empty template \u00b6 Each module will be a subclass of either a BaseModule or ThreadAwareModule . The latter supports running multiple actions in parallel, and so you should use a ThreadAwareModule when you will be processing multiple inputs that don't interact. For example, you may perform the same operation on multiple files in your local filesystem, or multiple disk snapshots in a cloud computing environment. Alternatively, you should use a BaseModule if you intend to only operate on a single entity, or if a system you interact with can handle its own parallel processing. A third type of module available is the PreflightModule . While functionaly identical to BaseModule the orchestration runs all PreflightModule s before standard modules. For example, you could use a PreflightModule to check for access to a cloud environment. Our example will be a ThreadAwareModule as the methods to be implemented are a superset of those for BaseModule . Our empty template starts as follows: class FileHashModule(module.ThreadAwareModule): def __init__(self, state, name, critical): \"\"\"Init.\"\"\" (FileHashModule, self).__init__(state, name=name, critical=critical) def GetThreadOnContainerType(self): \"\"\"What container type to thread on.\"\"\" def KeepThreadedContainersInState(self) -> bool: \"\"\"Keep, or pop the containers used for threading.\"\"\" def GetThreadPoolSize(self): \"\"\"Thread Pool Size.\"\"\" def SetUp(self): \"\"\"SetUp.\"\"\" def PreProcess(self): \"\"\"PreProcess.\"\"\" def Process(self, container): \"\"\"Processing.\"\"\" def PostProcess(self): \"\"\"PostProcessing.\"\"\" modules_manager.ModulesManager.RegisterModule(FileHashModule) In this template we have left out type checking for brevity, but if you wish to contribute your module upstream, you will need type annotations, checked by mypy, pytype and pylint. __init__() \u00b6 A standard class method, use this to declare any class members. GetThreadOnContainerType() \u00b6 Specific to ThreadAwareModule this method should return a container type that will be the basis of parallel processing. GetThreadPoolSize() \u00b6 Specific to ThreadAwareModule this method tells the orchestration how many parallel threads to use for this module's Process() method. KeepThreadedContainersInState() \u00b6 Specific to ThreadAwareModule this method returns True in the base class. If set to false by the child class, containers used for parallel processing will be popped from the state. SetUp() \u00b6 Called by the orchestration only once, with parameters as defined by the recipe file. PreProcess() \u00b6 Specific to ThreadAwareModule , this method is called exactly once by the orchestration between SetUp() and Process() . Process() \u00b6 This method performs the bulk of the processing action. This method is called for each module in parallel, based on dependencies as outlines by the recipe file. For a BaseModule no parameters will be passed in, but for a ThreadAwareModule the method will be called with a single container of the type specified in GetThreadOnContainerType() PostProcess() \u00b6 Specific to ThreadAwareModule , this method is called exactly once by the orchestration after Process() . 1 - Parallel Processing Helpers \u00b6 Three of the methods outlined above are used to tell the orchestration how to handle parallel processing. The GetThreadOnContainerType() method is used to identify which container is used for threading. That is, this is the container type that will get passed to Process(container) one at a time. Since we're operating on local filesystem files, we can use the existing container, containers.File . def GetThreadOnContainerType(self): return containers.File Second of these methods is GetThreadPoolSize() which tells the orchestration how many threads this module supports. Since we are doing local processing, we will set the number of threads to be based on the number of CPUs in the machine doing the work - halved to ensure we're not being greedy on local resources. def GetThreadPoolSize(self): count = math.floor(os.cpu_count() / 2) return 1 if count < 1 else count Finally, we want to operate on file containers, and only keep those file containers in the state if the hash is matched. We will do that by telling the orchestration to pop the containers when passed to Process() and add the matches back. def KeepThreadedContainersInState(self): return False 2 - Initialisation \u00b6 Two methods are used as part of module initialisation: The python class __init__() and DFTimewolf's SetUp() method. __init__ is simple enough, we only need to declare the class members: def __init__(self, state, name, critical): super(FileRegexModule, self).__init__(state, name=name, critical=critical) self.hashes = [] self.destination_dir = '' SetUp() is called by the orchestration with parameters as defined by the recipe file. Our module is going to receive two parameters: a comma separated list of files and a comma separated list of hashes. For any module that can take input in this manner should expect comma seperated values in a string rather than a list because it will likely come from a human on the CLI. def SetUp(self, paths, hashes, destination_directory): # Set the hashes self.hashes = hashes.split(',') # Files passed in should be added to the container store in the state. for p in paths.split(','): if p: filename = os.path.basename(p) self.StoreContainer(containers.File(name=filename, path=p)) 3 - Processing \u00b6 Now we arrive at the heavy lifting of our module, we have 3 methods remaining to implement. PreProcess is used for any actions that aren't considered part of the SetUp() but also need to be taken before processing. In an example that operates on a cloud platform, you could use this to create IAM permissions needed to perform the work. def PreProcess(self): if not os.access(self.destination_dir, os.W_OK): self.ModuleError( message=f'Cannot write to destination {self.destination_dir}, bailing out', critical=True) In our contrived example, we're going to test we have write permissions on the destination directory, and if not, we are raising a module error that will be handled by the orchestration. Next up is Process(container) . This method will receive one File container (as per GetThreadOnContainerType() ) and perform the check. def Process(container): BUF_SIZE = 65536 sha1 = hashlib.sha1() with open(container.path, 'rb') as f: while True: data = f.read(BUF_SIZE) if not data: break sha1.update(data) digest = sha1.hexdigest() if digest in self.hashes: os.rename(container.path, f'{self.destination_dir}/{container.name}') self.StoreContainer( containers.File( name=container.name, path=f'{self.destination_dir}/{container.name}')) Finally, we would implement a PostProcess() method. In our example, there is no great need for post processing, but for the sake of the example, we might have the following: def PostProcess(): count = len(self.GetContainers(containers.File)) if count == 0: self.ModuleError( message=f'No matching hashes found.', critical=False)","title":"Module writing basics"},{"location":"module-writing-basics/#how-to-write-a-dftimewolf-module","text":"The purpose of this guide is to walk a contributor through the basics of how to write a DFTimewolf module. Our example will perform the following: Receive filepaths of files on the local filesystem Hash the files and check against a list of hashes Move matching files to an output directory Keep only matching files in the state for processing by a later module","title":"How to write a DFTimewolf Module"},{"location":"module-writing-basics/#0-an-empty-template","text":"Each module will be a subclass of either a BaseModule or ThreadAwareModule . The latter supports running multiple actions in parallel, and so you should use a ThreadAwareModule when you will be processing multiple inputs that don't interact. For example, you may perform the same operation on multiple files in your local filesystem, or multiple disk snapshots in a cloud computing environment. Alternatively, you should use a BaseModule if you intend to only operate on a single entity, or if a system you interact with can handle its own parallel processing. A third type of module available is the PreflightModule . While functionaly identical to BaseModule the orchestration runs all PreflightModule s before standard modules. For example, you could use a PreflightModule to check for access to a cloud environment. Our example will be a ThreadAwareModule as the methods to be implemented are a superset of those for BaseModule . Our empty template starts as follows: class FileHashModule(module.ThreadAwareModule): def __init__(self, state, name, critical): \"\"\"Init.\"\"\" (FileHashModule, self).__init__(state, name=name, critical=critical) def GetThreadOnContainerType(self): \"\"\"What container type to thread on.\"\"\" def KeepThreadedContainersInState(self) -> bool: \"\"\"Keep, or pop the containers used for threading.\"\"\" def GetThreadPoolSize(self): \"\"\"Thread Pool Size.\"\"\" def SetUp(self): \"\"\"SetUp.\"\"\" def PreProcess(self): \"\"\"PreProcess.\"\"\" def Process(self, container): \"\"\"Processing.\"\"\" def PostProcess(self): \"\"\"PostProcessing.\"\"\" modules_manager.ModulesManager.RegisterModule(FileHashModule) In this template we have left out type checking for brevity, but if you wish to contribute your module upstream, you will need type annotations, checked by mypy, pytype and pylint.","title":"0 - An empty template"},{"location":"module-writing-basics/#__init__","text":"A standard class method, use this to declare any class members.","title":"__init__()"},{"location":"module-writing-basics/#getthreadoncontainertype","text":"Specific to ThreadAwareModule this method should return a container type that will be the basis of parallel processing.","title":"GetThreadOnContainerType()"},{"location":"module-writing-basics/#getthreadpoolsize","text":"Specific to ThreadAwareModule this method tells the orchestration how many parallel threads to use for this module's Process() method.","title":"GetThreadPoolSize()"},{"location":"module-writing-basics/#keepthreadedcontainersinstate","text":"Specific to ThreadAwareModule this method returns True in the base class. If set to false by the child class, containers used for parallel processing will be popped from the state.","title":"KeepThreadedContainersInState()"},{"location":"module-writing-basics/#setup","text":"Called by the orchestration only once, with parameters as defined by the recipe file.","title":"SetUp()"},{"location":"module-writing-basics/#preprocess","text":"Specific to ThreadAwareModule , this method is called exactly once by the orchestration between SetUp() and Process() .","title":"PreProcess()"},{"location":"module-writing-basics/#process","text":"This method performs the bulk of the processing action. This method is called for each module in parallel, based on dependencies as outlines by the recipe file. For a BaseModule no parameters will be passed in, but for a ThreadAwareModule the method will be called with a single container of the type specified in GetThreadOnContainerType()","title":"Process()"},{"location":"module-writing-basics/#postprocess","text":"Specific to ThreadAwareModule , this method is called exactly once by the orchestration after Process() .","title":"PostProcess()"},{"location":"module-writing-basics/#1-parallel-processing-helpers","text":"Three of the methods outlined above are used to tell the orchestration how to handle parallel processing. The GetThreadOnContainerType() method is used to identify which container is used for threading. That is, this is the container type that will get passed to Process(container) one at a time. Since we're operating on local filesystem files, we can use the existing container, containers.File . def GetThreadOnContainerType(self): return containers.File Second of these methods is GetThreadPoolSize() which tells the orchestration how many threads this module supports. Since we are doing local processing, we will set the number of threads to be based on the number of CPUs in the machine doing the work - halved to ensure we're not being greedy on local resources. def GetThreadPoolSize(self): count = math.floor(os.cpu_count() / 2) return 1 if count < 1 else count Finally, we want to operate on file containers, and only keep those file containers in the state if the hash is matched. We will do that by telling the orchestration to pop the containers when passed to Process() and add the matches back. def KeepThreadedContainersInState(self): return False","title":"1 - Parallel Processing Helpers"},{"location":"module-writing-basics/#2-initialisation","text":"Two methods are used as part of module initialisation: The python class __init__() and DFTimewolf's SetUp() method. __init__ is simple enough, we only need to declare the class members: def __init__(self, state, name, critical): super(FileRegexModule, self).__init__(state, name=name, critical=critical) self.hashes = [] self.destination_dir = '' SetUp() is called by the orchestration with parameters as defined by the recipe file. Our module is going to receive two parameters: a comma separated list of files and a comma separated list of hashes. For any module that can take input in this manner should expect comma seperated values in a string rather than a list because it will likely come from a human on the CLI. def SetUp(self, paths, hashes, destination_directory): # Set the hashes self.hashes = hashes.split(',') # Files passed in should be added to the container store in the state. for p in paths.split(','): if p: filename = os.path.basename(p) self.StoreContainer(containers.File(name=filename, path=p))","title":"2 - Initialisation"},{"location":"module-writing-basics/#3-processing","text":"Now we arrive at the heavy lifting of our module, we have 3 methods remaining to implement. PreProcess is used for any actions that aren't considered part of the SetUp() but also need to be taken before processing. In an example that operates on a cloud platform, you could use this to create IAM permissions needed to perform the work. def PreProcess(self): if not os.access(self.destination_dir, os.W_OK): self.ModuleError( message=f'Cannot write to destination {self.destination_dir}, bailing out', critical=True) In our contrived example, we're going to test we have write permissions on the destination directory, and if not, we are raising a module error that will be handled by the orchestration. Next up is Process(container) . This method will receive one File container (as per GetThreadOnContainerType() ) and perform the check. def Process(container): BUF_SIZE = 65536 sha1 = hashlib.sha1() with open(container.path, 'rb') as f: while True: data = f.read(BUF_SIZE) if not data: break sha1.update(data) digest = sha1.hexdigest() if digest in self.hashes: os.rename(container.path, f'{self.destination_dir}/{container.name}') self.StoreContainer( containers.File( name=container.name, path=f'{self.destination_dir}/{container.name}')) Finally, we would implement a PostProcess() method. In our example, there is no great need for post processing, but for the sake of the example, we might have the following: def PostProcess(): count = len(self.GetContainers(containers.File)) if count == 0: self.ModuleError( message=f'No matching hashes found.', critical=False)","title":"3 - Processing"},{"location":"recipe-caveat/","text":"Recipe caveats \u00b6 vt_pcap_ts \u00b6 This recipe will take a list of hashes (md5, sha256...) and check if they are known to Virustotal. If a hash is known to Virustotal, the recipe will check if there is one or more pcaps available for that hash from a sandbox run. Each available pcap will be downloaded and parsed. After the parsing, the data will be passed to Timesketch. Be careful, large pcap files might take a long time to process. To use this recipe, the user needs to provide an Virustotal Premium API key either via argument or in ~/.dftimewolfrc as vt_api_key Each parsed pcap will generate a new timeline in Timesketch in a given sketch.","title":"Recipe caveats"},{"location":"recipe-caveat/#recipe-caveats","text":"","title":"Recipe caveats"},{"location":"recipe-caveat/#vt_pcap_ts","text":"This recipe will take a list of hashes (md5, sha256...) and check if they are known to Virustotal. If a hash is known to Virustotal, the recipe will check if there is one or more pcaps available for that hash from a sandbox run. Each available pcap will be downloaded and parsed. After the parsing, the data will be passed to Timesketch. Be careful, large pcap files might take a long time to process. To use this recipe, the user needs to provide an Virustotal Premium API key either via argument or in ~/.dftimewolfrc as vt_api_key Each parsed pcap will generate a new timeline in Timesketch in a given sketch.","title":"vt_pcap_ts"},{"location":"recipe-list/","text":"Recipe list \u00b6 This is an auto-generated list of dfTimewolf recipes. To regenerate this list, from the repository root, run: poetry install -d python docs/generate_recipe_doc.py data/recipes aws_disk_to_gcp \u00b6 Copies EBS volumes from within AWS, and transfers them to GCP. Details: Copies EBS volumes from within AWS by pushing them to an AWS S3 bucket. The S3 bucket is then copied to a Google Cloud Storage bucket, from which a GCP Disk Image and finally a GCP Persistent Disk are created. This operation happens in the cloud and doesn't touch the local workstation on which the recipe is run. CLI parameters: Parameter Default value Description aws_region None AWS region containing the EBS volumes. gcp_zone None Destination GCP zone in which to create the disks. volumes None Comma separated list of EBS volume IDs (e.g. vol-xxxxxxxx). aws_bucket None AWS bucket for image storage. gcp_bucket None GCP bucket for image storage. --subnet None AWS subnet to copy instances from, required if there is no default subnet in the volume region. --gcp_project None Destination GCP project. --aws_profile None Source AWS profile. Modules: AWSVolumeSnapshotCollector , AWSSnapshotS3CopyCollector , S3ToGCSCopy , GCSToGCEImage , GCEDiskFromImage Module graph aws_forensics \u00b6 Copies a volume from an AWS account to an analysis VM. Details: Copies a volume from an AWS account, creates an analysis VM in AWS (with a startup script containing installation instructions for basic forensics tooling), and attaches the copied volume to it. CLI parameters: Parameter Default value Description remote_profile_name None Name of the AWS profile pointing to the AWS account where the volume(s) exist(s). remote_zone None The AWS zone in which the source volume(s) exist(s). incident_id None Incident ID to label the VM with. --instance_id None Instance ID of the instance to analyze. --volume_ids None Comma-separated list of volume IDs to copy. --all_volumes False Copy all volumes in the designated instance. Overrides volume_ids if specified. --boot_volume_size '50' The size of the analysis VM boot volume (in GB). --analysis_zone None The AWS zone in which to create the VM. --analysis_profile_name None Name of the AWS profile to use when creating the analysis VM. Modules: AWSCollector Module graph aws_logging_collect \u00b6 Collects logs from an AWS account and dumps the results to the filesystem. Details: Collects logs from an AWS account using a specified query filter and date ranges, and dumps them on the filesystem. If no args are provided this recipe will collect 90 days of logs for the default AWS profile. CLI parameters: Parameter Default value Description region None AWS Region --profile_name None Name of the AWS profile to collect logs from. --query_filter None Filter expression to use to query logs. --start_time None Start time for the query. --end_time None End time for the query. Modules: AWSLogsCollector Module graph aws_logging_ts \u00b6 Collects logs from an AWS account, processes the logs with Plaso and uploads the result to Timesketch. Details: Collects logs from an AWS account using a specified query filter and date ranges, processes the logs with plaso and uploads the result to Timesketch. If no args are provided this recipe will collect 90 days of logs for the default AWS profile. CLI parameters: Parameter Default value Description region None AWS Region --profile_name None Name of the AWS profile to collect logs from. --query_filter None Filter expression to use to query logs. --start_time None Start time for the query. --end_time None End time for the query. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: AWSLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph aws_turbinia_ts \u00b6 Copies EBS volumes from within AWS, transfers them to GCP, analyses with Turbinia and exports the results to Timesketch. Details: Copies EBS volumes from within AWS, uses buckets and cloud-to-cloud operations to transfer the data to GCP. Once in GCP, a persistent disk is created and a job is added to the Turbinia queue to start analysis. The resulting Plaso file is then exported to Timesketch. CLI parameters: Parameter Default value Description aws_region None AWS region containing the EBS volumes. gcp_zone None Destination GCP zone in which to create the disks. volumes None Comma separated list of EBS volume IDs (e.g. vol-xxxxxxxx). aws_bucket None AWS bucket for image storage. gcp_bucket None GCP bucket for image storage. --subnet None AWS subnet to copy instances from, required if there is no default subnet in the volume region. --gcp_project None Destination GCP project. --aws_profile None Source AWS profile. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --request_ids None Comma separated Turbinia request identifiers to process. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --turbinia_zone 'us-central1-f' Zone Turbinia is located in --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. Modules: AWSVolumeSnapshotCollector , AWSSnapshotS3CopyCollector , S3ToGCSCopy , GCSToGCEImage , GCEDiskFromImage , TurbiniaGCPProcessor , TimesketchExporter Module graph azure_forensics \u00b6 Copies a disk from an Azure account to an analysis VM. Details: Copies a disk from an Azure account, creates an analysis VM in Azure (with a startup script containing installation instructions for basic forensics tooling), and attaches the copied disk to it. CLI parameters: Parameter Default value Description remote_profile_name None Name of the Azure profile pointing to the Azure account where the disk(s) exist(s). analysis_resource_group_name None The Azure resource group name in which to create the VM. incident_id None Incident ID to label the VM with. ssh_public_key None A SSH public key string to add to the VM (e.g. ssh-rsa AAAAB3NzaC1y... ). --instance_name None Instance name of the instance to analyze. --disk_names None Comma-separated list of disk names to copy. --all_disks False Copy all disks in the designated instance. Overrides disk_names if specified. --boot_disk_size '50' The size of the analysis VM's boot disk (in GB). --analysis_region None The Azure region in which to create the VM. --analysis_profile_name None Name of the Azure profile to use when creating the analysis VM. Modules: AzureCollector Module graph azure_logging_collect \u00b6 Collects logs from an Azure subscription and dumps the results to the filesystem. Details: Collects logs from an Azure subscription using a specified filter, and dumps them on the filesystem. CLI parameters: Parameter Default value Description subscription_id None Subscription ID for the subscription to collect logs from. filter_expression None A filter expression to use for the log query, must specify at least a start date like \"eventTimestamp ge '2022-02-01'\" --profile_name None A profile name to use when looking for Azure credentials. Modules: AzureLogsCollector Module graph azure_logging_ts \u00b6 Collects logs from an Azure subscription, processes the logs with Plaso and uploads the result to Timesketch. Details: Collects logs from an Azure subscription using a specified query filter and date ranges, processes the logs with plaso and uploads the result to Timesketch. CLI parameters: Parameter Default value Description subscription_id None Subscription ID for the subscription to collect logs from. filter_expression None A filter expression to use for the log query, must specify at least a start date like \"eventTimestamp ge '2022-02-01'\" --profile_name None A profile name to use when looking for Azure credentials. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: AzureLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph bigquery_collect \u00b6 Collects results from BigQuery and dumps them on the filesystem. Details: Collects results from BigQuery in a GCP project and dumps them in JSONL on the local filesystem. CLI parameters: Parameter Default value Description project_name None Name of GCP project to collect logs from. query None Query to execute. description None Human-readable description of the query. Modules: BigQueryCollector Module graph bigquery_ts \u00b6 Collects results from BigQuery and uploads them to Timesketch. Details: Collects results from BigQuery in JSONL form, dumps them to the filesystem, and uploads them to Timesketch. CLI parameters: Parameter Default value Description project_name None Name of GCP project to collect logs from. query None Query to execute. description None Human-readable description of the query. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: BigQueryCollector , TimesketchExporter Module graph gce_disk_copy \u00b6 Copy disks from one project to another. Details: Copies disks from one project to another. The disks can be specified individually, or instances can be specified, to copy all their disks or boot disks. CLI parameters: Parameter Default value Description source_project_name None Source project containing the disks to export. --destination_project_name None Project to where the disk images are exported. If not provided, source_project_name is used. --source_disk_names None Comma-separated list of disk names to export. If not provided, disks attached to remote_instance_name will be used. --remote_instance_names None Comma-separated list of instances in source project from which to copy disks. If not provided, disk_names will be used. --all_disks False If True, copy all disks attached to the remote_instance_names instances. If False and remote_instance_name is provided, it will select the instance's boot disk. --zone 'us-central1-f' Destination zone for the disks to be copied to. --stop_instances False Stop instances after disks have been copied Modules: GCEDiskCopy Module graph gce_disk_export \u00b6 Export a disk image from a GCP project to a Google Cloud Storage bucket. Details: Creates a disk image from Google Compute persistent disks, compresses the images, and exports them to Google Cloud Storage. The exported images names are appended by .tar.gz. As this export happens through a Cloud Build job, the default service account [PROJECT-NUMBER]@cloudbuild.gserviceaccount.com in the source or analysis project (if provided) must have the IAM role [Storage Admin] on their corresponding project's storage bucket/folder. CLI parameters: Parameter Default value Description source_project_name None Source project containing the disk to export. gcs_output_location None Google Cloud Storage parent bucket/folder to which to export the image. --analysis_project_name None Project where the disk image is created then exported. If not provided, the image is exported to a bucket in the source project. --source_disk_names None Comma-separated list of disk names to export. If not provided, disks attached to remote_instance_name will be used. --remote_instance_name None Instance in source project to export its disks. If not provided, disk_names will be used. --all_disks False If True, copy all disks attached to the remote_instance_name instance. If False and remote_instance_name is provided, it will select the instance's boot disk. --exported_image_name None Name of the output file, must comply with ^[A-Za-z0-9-]*$ and '.tar.gz' will be appended to the name. If not provided or if more than one disk is selected, the exported image will be named exported-image-{TIMESTAMP('YYYYmmddHHMMSS')} . --image_format '' An image format to use. Example values: qcow2, vmdk. Default (empty value) will be .tar.gz. Modules: GoogleCloudDiskExport Module graph gce_disk_export_dd \u00b6 Stream the disk bytes from a GCP project to a Google Cloud Storage bucket. Details: The export is performed via bit streaming the the disk bytes to GCS. This will allow getting a disk image out of the project in case both organization policies constraints/compute.storageResourceUseRestrictions and constraints/compute.trustedImageProjects are enforced and in case OsLogin is allowed only for the organization users while the analyst is an external user with no roles/ compute.osLoginExternalUser role. The exported images names are appended by .tar.gz. The compute engine default service account in the source project must have sufficient permissions to Create and List Storage objects on the corresponding storage bucket/folder. CLI parameters: Parameter Default value Description source_project_name None Source project containing the disk to export. gcs_output_location None Google Cloud Storage parent bucket/folder to which to export the image. --source_disk_names None Comma-separated list of disk names to export. If not provided, disks attached to remote_instance_name will be used. --remote_instance_name None Instance in source project to export its disks. If not provided, source_disk_names will be used. --all_disks False If True, copy all disks attached to the remote_instance_name instance. If False and remote_instance_name is provided, it will select the instance's boot disk. --boot_image_project 'debian-cloud' Name of the project where the boot disk image of the export VM is stored. --boot_image_family 'debian-10' Name of the image to use to create the boot disk of the export VM. Modules: GoogleCloudDiskExportStream Module graph gcp_cloud_resource_tree \u00b6 Generates a parent/children tree for given GCP resource. Details: Generates a parent/children tree for given GCP resource by enumerating all the currently available resources. It also will attempt to fill any gaps identified in the tree through querying the GCP logs CLI parameters: Parameter Default value Description project_id None ID of the project where the resource is located location None Resource location (zone/region) or 'global' resource_type None Resource type (currently supported types: gce_instance, gce_disk, gce_image, gce_machine_image, gce_instance_template, gce_snapshot) --resource_id None Resource id --resource_name None Resource name Modules: GCPCloudResourceTree Module graph gcp_cloud_resource_tree_offline \u00b6 Generates a parent/children tree for given GCP resource using the supplied exported GCP logs Details: Generates a parent/children tree for given GCP resource using the supplied exported GCP logs CLI parameters: Parameter Default value Description project_id None ID of the project where the resource is located location None Resource location (zone/region) or 'global' resource_type None Resource type (currently supported types: gce_instance, gce_disk, gce_image, gce_machine_image, gce_instance_template, gce_snapshot) paths None Comma-separated paths to GCP log files. Log files should contain log entiries in json format. --resource_id None Resource id --resource_name None Resource name Modules: FilesystemCollector , GCPCloudResourceTree Module graph gcp_forensics \u00b6 Copies disk from a GCP project to an analysis VM. Details: Copies a persistent disk from a GCP project to another, creates an analysis VM (with a startup script containing installation instructions for basic forensics tooling) in the destination project, and attaches the copied GCP persistent disk to it. CLI parameters: Parameter Default value Description source_project_name None Name of the project containing the instance / disks to copy. analysis_project_name None Name of the project where the analysis VM will be created and disks copied to. --incident_id None Incident ID to label the VM with. --instances None Name of the instance to analyze. --disks None Comma-separated list of disks to copy from the source GCP project (if instance not provided). --all_disks False Copy all disks in the designated instance. Overrides disk_names if specified. --stop_instances False Stop the designated instance after copying disks. --create_analysis_vm True Create an analysis VM in the destination project. --cpu_cores '4' Number of CPU cores of the analysis VM. --boot_disk_size '50' The size of the analysis VM boot disk (in GB). --boot_disk_type 'pd-standard' Disk type to use [pd-standard, pd-ssd]. --zone 'us-central1-f' The GCP zone where the Analysis VM and copied disks will be created. --analysis_vm_name 'gcp-forensics-vm' Name (prefix) to give the analysis vm. Modules: GCEDiskCopy , GCEForensicsVM Module graph gcp_logging_cloudaudit_ts \u00b6 Collects GCP logs from a project and exports them to Timesketch. Details: Collects GCP logs from a project and exports them to Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. start_date None Start date. end_date None End date. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph gcp_logging_cloudsql_ts \u00b6 Collects GCP related to Cloud SQL instances in a project and exports them to Timesketch. Details: Collects GCP related to Cloud SQL instances in a project and exports them to Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. start_date None Start date. end_date None End date. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph gcp_logging_collect \u00b6 Collects logs from a GCP project and dumps on the filesystem (JSON). https://cloud.google.com/logging/docs/view/query-library for example queries. Details: Collects logs from a GCP project and dumps on the filesystem. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. filter_expression \"resource.type = 'gce_instance'\" Filter expression to use to query GCP logs. See https://cloud.google.com/logging/docs/view/query-library for examples. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector Module graph gcp_logging_gce_instance_ts \u00b6 GCP Instance Cloud Audit logs to Timesketch Details: Collects GCP Cloud Audit Logs for a GCE instance and exports them to Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. instance_id None Identifier for GCE instance (Instance ID). --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph gcp_logging_gce_ts \u00b6 Loads all GCE Cloud Audit Logs in a GCP project into Timesketch. Details: Loads all GCE Cloud Audit Logs for all instances in a GCP project into Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. start_date None Start date. end_date None End date. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph gcp_logging_ts \u00b6 Collects logs from a GCP project and sends them to Timesketch. Details: Collects logs from a GCP project and sends them to Timesketch. https://cloud.google.com/logging/docs/view/query-library for example queries. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. filter_expression \"resource.type = 'gce_instance'\" Filter expression to use to query GCP logs. See https://cloud.google.com/logging/docs/view/query-library for examples. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits --analyzers None Timesketch analyzers to run. --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --incident_id None Incident ID (used for Timesketch description). --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: GCPLogsCollector , GCPLoggingTimesketch , TimesketchExporter Module graph gcp_turbinia_disk_copy_ts \u00b6 Imports a remote GCP persistent disk, processes it with Turbinia and sends results to Timesketch. Details: Imports a remote GCP persistent disk into an analysis GCP project and sends the result of Turbinia processing to Timesketch. Copies a disk from a remote GCP project into an analysis project Creates Turbinia processing request to process the imported disk Downloads and sends results of the Turbinia processing to Timesketch. This recipe will also start an analysis VM in the destination project with the attached disk (the same one that Turbinia will have processed). If the target disk is already in the same project as Turbinia, you can use the gcp_turbinia_ts recipe. CLI parameters: Parameter Default value Description source_project_name None Name of the project containing the instance / disks to copy. analysis_project_name None Name of the project containing the Turbinia instance. --request_ids None Comma separated Turbinia request identifiers to process. --yara_rules_path None Paths to Yara rules sent to Turbinia for processing. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --turbinia_zone 'us-central1-f' The GCP zone the disk to process and Turbinia workers are in. --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --incident_id None Incident ID (used for Timesketch description and to label the VM with). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --create_analysis_vm True Create an analysis VM in the destination project. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --instances None Name of the instances to analyze. --disks None Comma-separated list of disks to copy from the source GCP project (if instance not provided). --all_disks False Copy all disks in the designated instance. Overrides disk_names if specified. --stop_instances False Stop the designated instances after copying disks. --cpu_cores '4' Number of CPU cores of the analysis VM. --boot_disk_size '50' The size of the analysis VM boot disk (in GB). --boot_disk_type 'pd-standard' Disk type to use [pd-standard, pd-ssd] --image_project 'ubuntu-os-cloud' Name of the project where the analysis VM image is hosted. --image_family 'ubuntu-2204-lts' Name of the image to use to create the analysis VM. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. --analysis_vm_name 'gcp-forensics-vm' Name (prefix) to give the analysis vm. Modules: LocalYaraCollector , GCEDiskCopy , GCEForensicsVM , TurbiniaGCPProcessor , TimesketchExporter Module graph gcp_turbinia_ts \u00b6 Processes existing GCP persistent disks with Turbinia project and sends results to Timesketch. Details: Process GCP persistent disks with Turbinia and send output to Timesketch. This processes disks that are already in the project where Turbinia exists. If you want to copy disks from another project, use the gcp_turbinia_disk_copy_ts recipe. CLI parameters: Parameter Default value Description analysis_project_name None Name of GCP project the disk exists in. turbinia_zone None The GCP zone the disk to process (and Turbinia workers) are in. --disk_names None Comma separated names of GCP persistent disks to process. This parameter can only be used if --request_ids is not provided. --request_ids None Comma separated Turbinia request identifiers to process. This parameter can only be used if --disk_names is not provided. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. Modules: TurbiniaGCPProcessor , TimesketchExporter Module graph gdrive_collect \u00b6 Collect files from Google Drive. Details: Collect files from Google Drive. CLI parameters: Parameter Default value Description --folder_id None Parent Google drive folder ID to download from. Mutually exclusive with drive_ids. --recursive False Recursively collect from folder_id --drive_ids None Comma-separated list of drive IDs to download. Mutually exclusive with folder_id. --output_directory None Output directory for collected files. --overwrite_existing False Overwrite existing files. --max_download_workers 5 Maximum number of worker threads to use for downloading files. Modules: GoogleDriveCollector Module graph grr_artifact_ts \u00b6 Fetches default ForensicArtifacts from a sequence of GRR hosts, processes them with plaso, and sends the results to Timesketch. Details: Collect artifacts from hosts using GRR. Collect a predefined list of artifacts from hosts using GRR Process them with a local install of plaso Export them to a Timesketch sketch. The default set of artifacts is defined in the GRRArtifactCollector module (see the _DEFAULT_ARTIFACTS_* class attributes in grr_hosts.py ), and varies per platform. CLI parameters: Parameter Default value Description hostnames None Comma-separated list of hostnames or GRR client IDs to process. reason None Reason for collection. --artifacts None Comma-separated list of artifacts to fetch (override default artifacts). --extra_artifacts None Comma-separated list of artifacts to append to the default artifact list. --use_raw_filesystem_access False Use raw disk access to fetch artifacts. --approvers None Emails for GRR approval request. --sketch_id None Timesketch sketch to which the timeline should be added. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --analyzers None Timesketch analyzers to run --token_password '' Optional custom password to decrypt Timesketch credential file with. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --incident_id None Incident ID (used for Timesketch description). --grr_server_url 'http://localhost:8000' GRR endpoint. --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --user_docker True Whether the LocalPlasoProcessor should use Docker or not. --max_file_size '5368709120' Maximum size of files to collect (in bytes). Modules: GRRArtifactCollector , LocalPlasoProcessor , TimesketchExporter Module graph grr_files_collect \u00b6 Collects specific files from one or more GRR hosts. Details: Collects specific files from one or more GRR hosts. Files can be a glob pattern (e.g. /tmp/*.so ) and support GRR variable interpolation (e.g. %%users.localappdata%%/Directory/ ) CLI parameters: Parameter Default value Description hostnames None Comma-separated list of hostnames or GRR client IDs to process. reason None Reason for collection. files None Comma-separated list of files to fetch (supports globs and GRR variable interpolation). directory None Directory in which to export files. --use_raw_filesystem_access False Use raw disk access to fetch artifacts. --approvers None Emails for GRR approval request. --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --action 'download' String denoting action (download/hash/stat) to take --grr_server_url 'http://localhost:8000' GRR endpoint --grr_username 'admin' GRR username --grr_password 'admin' GRR password --max_file_size '5368709120' Maximum size of files to collect (in bytes). Modules: GRRFileCollector , LocalFilesystemCopy Module graph grr_flow_collect \u00b6 Download the result of a GRR flow to the local filesystem. Details: Download the result of a GRR flow to the local filesystem. Flow IDs are unique per client , so both need to be provided in sequence. CLI parameters: Parameter Default value Description hostnames None Hostname(s) to collect the flow from. flow_ids None Flow ID(s) to download. reason None Reason for collection. directory None Directory in which to export files. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'admin' GRR password Modules: GRRFlowCollector , LocalFilesystemCopy Module graph grr_hunt_artifacts \u00b6 Starts a GRR hunt for the default set of artifacts. Details: Starts a GRR artifact hunt and provides the Hunt ID to the user. Feed the Hunt ID to grr_huntresults_ts to process results through Plaso and export them to Timesketch. CLI parameters: Parameter Default value Description artifacts None Comma-separated list of artifacts to hunt for. reason None Reason for collection. --use_raw_filesystem_access False Use raw disk access to fetch artifacts. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --max_file_size '5368709120' Maximum size of files to collect (in bytes). --match_mode None Match mode of the client rule set (ANY or ALL) --client_operating_systems None Comma-separated list of client operating systems to filter hosts on (linux, osx, win). --client_labels None Comma-separated list of client labels to filter GRR hosts on. Modules: GRRHuntArtifactCollector Module graph grr_hunt_file \u00b6 Starts a GRR hunt for a list of files. Details: Starts a GRR hunt for a list of files and provides a Hunt ID to the user. Feed the Hunt ID to grr_huntresults_ts to process results through Plaso and export them to Timesketch. Like in grr_files_collect , files can be globs and support variable interpolation. CLI parameters: Parameter Default value Description file_path_list None Comma-separated list of file paths to hunt for. reason None Reason for collection. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --max_file_size '5368709120' Maximum size of files to collect (in bytes). --match_mode None Match mode of the client rule set (ANY or ALL) --client_operating_systems None Comma-separated list of client operating systems to filter hosts on (linux, osx, win). --client_labels None Comma-separated list of client labels to filter GRR hosts on. Modules: GRRHuntFileCollector Module graph grr_hunt_osquery \u00b6 Starts a GRR hunt for an Osquery flow. Details: Starts a GRR osquery hunt and provides the Hunt ID to the user. CLI parameters: Parameter Default value Description reason None Reason for collection. --osquery_query None Osquery query to hunt for. --osquery_paths None Path(s) to text file containing one osquery query per line. --remote_configuration_path '' Path to a remote osquery configuration file on the GRR client. --local_configuration_path '' Path to a local osquery configuration file. --configuration_content '' Osquery configuration as a JSON string. --file_collection_columns '' The file collection columns. --timeout_millis '300000' Osquery timeout in milliseconds --ignore_stderr_errors False Ignore osquery stderr errors --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --match_mode None Match mode of the client rule set (ANY or ALL) --client_operating_systems None Comma-separated list of client operating systems to filter hosts on (linux, osx, win). --client_labels None Comma-separated list of client labels to filter GRR hosts on. Modules: OsqueryCollector , GRRHuntOsqueryCollector Module graph grr_huntresults_ts \u00b6 Fetches the results of a GRR hunt, processes them with Plaso, and exports the results to Timesketch. Details: Download the results of a GRR hunt and process them. Collect results of a hunt given its Hunt ID Processes results with a local install of Plaso Exports processed items to a new Timesketch sketch CLI parameters: Parameter Default value Description hunt_id None ID of GRR Hunt results to fetch. reason None Reason for exporting hunt (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password Modules: GRRHuntDownloader , LocalPlasoProcessor , TimesketchExporter Module graph grr_osquery_flow \u00b6 Runs osquery on GRR hosts and save any results to local CSV files. Details: Runs osquery on GRR hosts and save any results to local CSV files. CLI parameters: Parameter Default value Description reason None Reason for collection. hostnames None Hostname(s) to collect the osquery flow from. --osquery_query None Osquery query to hunt for. --osquery_paths None Path(s) to text file containing one osquery query per line. --remote_configuration_path '' Path to a remote osquery configuration file on the GRR client. --local_configuration_path '' Path to a local osquery configuration file. --configuration_content '' Osquery configuration as a JSON string. --file_collection_columns '' The file collection columns. --timeout_millis '300000' Osquery timeout in milliseconds --ignore_stderr_errors False Ignore osquery stderr errors --directory None Directory in which to export results. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'admin' GRR password Modules: OsqueryCollector , GRROsqueryCollector Module graph grr_timeline_ts \u00b6 Runs a TimelineFlow on a set of GRR hosts, generating a filesystem bodyfile for each host. These bodyfiles are processed results with Plaso, and the resulting plaso files are exported to Timesketch. Details: Uses the GRR TimelineFlow to generate a filesystem timeline and exports it to Timesketch.. CLI parameters: Parameter Default value Description hostnames None Comma-separated list of hostnames or GRR client IDs to process. root_path '/' Root path for timeline generation. reason None Reason for collection. --skip_offline_clients False Whether to skip clients that are offline. --approvers None Comma-separated list of usernames to ask for approval. --sketch_id None Timesketch sketch to which the timeline should be added. --grr_server_url 'http://localhost:8000' GRR endpoint. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --grr_username 'admin' GRR username. --grr_password 'admin' GRR password. --user_docker True Whether the LocalPlasoProcessor should use Docker or not. Modules: GRRTimelineCollector , LocalPlasoProcessor , TimesketchExporter Module graph grr_yarascan \u00b6 Run Yara rules on hosts memory. Details: Run Yara rules on hosts memory. CLI parameters: Parameter Default value Description reason None Reason for collection. hostnames None Hostname(s) to collect the flow from. --yara_name_filter None Filter to filter Yara sigs by. --dump_process_on_match False Whether to dump the process on match. --api_key None API Key to the Yeti instance --api_root 'http://localhost/api/' API root of the Yeti instance (e.g. http://localhost/api/) --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'demo' GRR password Modules: YetiYaraCollector , GRRYaraScanner Module graph gsheets_ts \u00b6 Collects data from google sheets and outputs them to Timesketch. Details: Collects data from google sheets and outputs them to Timesketch. CLI parameters: Parameter Default value Description spreadsheet None ID or URL of the Google Sheet spreadsheet to collect data from. --sheet_names [] Comma-separated list sheet names to collect date from. If not set all sheets in the spreadsheet will be parsed. --validate_columns True Set to True to check for mandatory columns required by Timesketch while extracting data. Set to False to ignore validation. Default is True. --sketch_id None Sketch to which the timeline should be added --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with --incident_id None Incident ID (used for Timesketch description) --wait_for_timelines True Whether to wait for timelines to finish processing. Modules: GoogleSheetsCollector , TimesketchExporter Module graph openrelik_ts \u00b6 Processes files from the local file system using OpenRelik. Sends the results to Timesketch. Details: Processes files from the local file system using OpenRelik. Sends the results to Timesketch. CLI parameters: Parameter Default value Description paths None Comma-separated list of paths to process. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --folder_id None OpenRelik Folder ID. --template_workflow_id None OpenRelik workflow template ID. --openrelik_api 'http://localhost:8710' OpenRelik API server URI. --openrelik_ui 'http://localhost:8711' OpenRelik UI URI. --openrelik_api_key '' OpenRelik API key --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --analyzers None Timesketch analyzers to run --token_password '' Optional custom password to decrypt Timesketch credential file with. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. Modules: FilesystemCollector , OpenRelikProcessor , TimesketchExporter Module graph plaso_ts \u00b6 Processes a list of file paths using a Plaso and export results to Timesketch. Details: Processes a list of file paths using Plaso and sends results to Timesketch. Collectors collect from a path in the FS Processes them with a local install of plaso Exports them to a new Timesketch sketch CLI parameters: Parameter Default value Description paths None Comma-separated list of paths to process. --analyzers None Timesketch analyzers to run --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: FilesystemCollector , LocalPlasoProcessor , TimesketchExporter Module graph ts_collect \u00b6 Collects Timesketch events. Details: Collects Timesketch events to a local file. CLI parameters: Parameter Default value Description directory None Directory in which to export files. query_string '*' The query string. Defaults to '*' (all events). --sketch_id None the Timesketch sketch ID. --start_datetime None The start datetime. --end_datetime None The end datetime. --indices None The comma-separated Timesketch indices. --labels None the comma-separated Timesketch event labels. --output_format 'csv' The output format (csv/json/jsonl). Defaults to csv --include_internal_columns False Include internal Timesketch fields in output. Defaults to false. --return_fields '*' The Timesketch fields to return from the search query. Defaults to *. --search_name None The search name (used as a filename prefix). --token_password '' Optional custom password to decrypt Timesketch credential file with. --timesketch_endpoint None The Timesketch endpoint URL --timesketch_username None The Timesketch username. --timesketch_password None The Timesketch password. Modules: TimesketchSearchEventCollector , LocalFilesystemCopy Module graph upload_gdrive \u00b6 Uploads local file(s) to Google Drive. Details: Uploads file(s) to Google Drive. CLI parameters: Parameter Default value Description files None Comma-separated list of paths to files. --parent_folder_id '0B0m2ov3yrR0bNzBhNjcxNjctN2EyYS00ZTgxLTk3YjgtMTI5NGI1OGI3MTJh' --new_folder_name None Optional new folder name. --max_upload_workers 5 Maximum number of worker threads to use for uploading files. Modules: FilesystemCollector , GoogleDriveExporter Module graph upload_ts \u00b6 Uploads a local CSV or Plaso file to Timesketch. Details: Uploads a CSV or Plaso file to Timesketch. CLI parameters: Parameter Default value Description files None Comma-separated list of paths to CSV files or Plaso storage files. --analyzers None Timesketch analyzers to run. --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --incident_id None Incident ID (used for Timesketch description). --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: FilesystemCollector , TimesketchExporter Module graph upload_turbinia \u00b6 Uploads arbitrary files to Turbinia and downloads results. Details: Uploads arbitrary files to Turbinia for processing. The recipe will wait for Turbinia to return with results and will download them back to the filesystem. The Turbinia system needs to be accessible via SSH. CLI parameters: Parameter Default value Description files None Paths to process. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --destination_turbinia_dir None Destination path in Turbinia host to write the files to. --hostname None Remote host. --directory None Directory in which to copy and compress files. --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --local_turbinia_results None Directory where Turbinia results will be downloaded to. --turbinia_zone 'us-central1-f' The GCP zone the disk to process and Turbinia workers are in. --sketch_id None Timesketch sketch ID. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. Modules: FilesystemCollector , LocalFilesystemCopy , TurbiniaArtifactProcessor Module graph upload_web_ts \u00b6 Uploads a CSV/JSONL or Plaso file to Timesketch and runs web-related Timesketch analyzers. Details: Uploads a CSV or Plaso file to Timesketch and runs a series of web-related analyzers on the uploaded data. The following analyzers will run on the processed timeline: browser_search,browser_timeframe,account_finder,phishy_domains,evtx_gap,login,win_crash,safebrowsing,chain . CLI parameters: Parameter Default value Description files None Comma-separated list of paths to CSV files or Plaso storage files. --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --incident_id None Incident ID (used for Timesketch description). --wait_for_analyzers True Wait for analyzers until they complete their run, if set to False the TS enhancer will be skipped. --searches_to_skip None A comma separated list of saved searches that should not be uploaded. --analyzer_max_checks '0' Number of wait cycles (per cycle is 3 seconds) before terminating wait for analyzers to complete. --aggregations_to_skip None A comma separated list of aggregation names that should not be uploaded. Modules: FilesystemCollector , TimesketchExporter Module graph vt_evtx \u00b6 Downloads the EVTX files from VirusTotal for a specific hash. Details: Downloads the EVTX files from VirusTotal sandbox run for a specific hash, processes it with Plaso. CLI parameters: Parameter Default value Description hashes None Comma-separated list of hashes to process. directory None Directory in which to export files. --vt_api_key 'admin' Virustotal API key Modules: VTCollector , LocalPlasoProcessor Module graph vt_evtx_ts \u00b6 Downloads the EVTX from VirusTotal sandbox runs for a specific hash and uploads the corresponding timeline to Timesketch. Details: Downloads the EVTX file generated by VirusTotal during the sandbox runs for a specific hash, processes the EVTX files with Plaso and uploads the resulting Plaso file to Timesketch. CLI parameters: Parameter Default value Description hashes None Comma-separated list of hashes to process. directory None Directory in which to export files. --vt_api_key 'admin' Virustotal API key --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: VTCollector , LocalPlasoProcessor , TimesketchExporter Module graph vt_pcap \u00b6 Downloads the PCAP from VirusTotal for a specific hash. Details: Downloads the PCAP files generated from VirusTotal sandbox's run for a specific hash. CLI parameters: Parameter Default value Description hashes None Comma-separated list of hashes to process. directory None Directory in which to export files. --vt_api_key 'admin' Virustotal API key Modules: VTCollector , LocalFilesystemCopy Module graph workspace_logging_collect \u00b6 Collects Workspace Audit logs and dumps them on the filesystem. Details: Collects logs from Workspace Audit log and dumps them on the filesystem. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list#ApplicationName for a list of application names. For filters, see https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list. CLI parameters: Parameter Default value Description application_name None Name of application to to collect logs for. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list#ApplicationName for a list of possible values. --user 'all' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list. Modules: WorkspaceAuditCollector Module graph workspace_meet_ts \u00b6 Collects Meet records and adds them to Timesketch Details: Collects Google Workspace audit records or a Google Meet and adds them to Timesketch. CLI parameters: Parameter Default value Description meeting_id None ID for the Meeting to look up. (Without the '-' delimiter) --start_time None Start time. --end_time None End time. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph workspace_user_activity_ts \u00b6 Collects records for a Google Workspace user and adds them to Timesketch Details: Collects records for a Google Workspace user and adds them to Timesketch. Collects logs for the following apps: Login , Drive , Token , Chrome , CAA , DataStudio , GroupsEnterprise , Calendar , Chat , Groups , Meet , UserAccounts . CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time (yyyy-mm-ddTHH:MM:SSZ). --end_time None End time (yyyy-mm-ddTHH:MM:SSZ). --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector-Login , WorkspaceAuditCollector-Drive , WorkspaceAuditCollector-Token , WorkspaceAuditCollector-Chrome , WorkspaceAuditCollector-CAA , WorkspaceAuditCollector-DataStudio , WorkspaceAuditCollector-GroupsEnterprise , WorkspaceAuditCollector-Calendar , WorkspaceAuditCollector-Chat , WorkspaceAuditCollector-GCP , WorkspaceAuditCollector-Groups , WorkspaceAuditCollector-Meet , WorkspaceAuditCollector-UserAccounts , WorkspaceAuditTimesketch , TimesketchExporter Module graph workspace_user_device_ts \u00b6 Collects mobile records and adds to Timesketch Details: Collects mobile (Device Audit activity) records for a Workspace user and adds them to Timesketch. CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph workspace_user_drive_ts \u00b6 Collects Drive records for a Workspace user and adds them to Timesketch Details: Collects Drive records for a Workspace user and adds them to Timesketch. CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph workspace_user_login_ts \u00b6 Collects login records and adds to Timesketch Details: Collects login records for a Workspace user and adds them to Timesketch. CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph","title":"Recipe list"},{"location":"recipe-list/#recipe-list","text":"This is an auto-generated list of dfTimewolf recipes. To regenerate this list, from the repository root, run: poetry install -d python docs/generate_recipe_doc.py data/recipes","title":"Recipe list"},{"location":"recipe-list/#aws_disk_to_gcp","text":"Copies EBS volumes from within AWS, and transfers them to GCP. Details: Copies EBS volumes from within AWS by pushing them to an AWS S3 bucket. The S3 bucket is then copied to a Google Cloud Storage bucket, from which a GCP Disk Image and finally a GCP Persistent Disk are created. This operation happens in the cloud and doesn't touch the local workstation on which the recipe is run. CLI parameters: Parameter Default value Description aws_region None AWS region containing the EBS volumes. gcp_zone None Destination GCP zone in which to create the disks. volumes None Comma separated list of EBS volume IDs (e.g. vol-xxxxxxxx). aws_bucket None AWS bucket for image storage. gcp_bucket None GCP bucket for image storage. --subnet None AWS subnet to copy instances from, required if there is no default subnet in the volume region. --gcp_project None Destination GCP project. --aws_profile None Source AWS profile. Modules: AWSVolumeSnapshotCollector , AWSSnapshotS3CopyCollector , S3ToGCSCopy , GCSToGCEImage , GCEDiskFromImage Module graph","title":"aws_disk_to_gcp"},{"location":"recipe-list/#aws_forensics","text":"Copies a volume from an AWS account to an analysis VM. Details: Copies a volume from an AWS account, creates an analysis VM in AWS (with a startup script containing installation instructions for basic forensics tooling), and attaches the copied volume to it. CLI parameters: Parameter Default value Description remote_profile_name None Name of the AWS profile pointing to the AWS account where the volume(s) exist(s). remote_zone None The AWS zone in which the source volume(s) exist(s). incident_id None Incident ID to label the VM with. --instance_id None Instance ID of the instance to analyze. --volume_ids None Comma-separated list of volume IDs to copy. --all_volumes False Copy all volumes in the designated instance. Overrides volume_ids if specified. --boot_volume_size '50' The size of the analysis VM boot volume (in GB). --analysis_zone None The AWS zone in which to create the VM. --analysis_profile_name None Name of the AWS profile to use when creating the analysis VM. Modules: AWSCollector Module graph","title":"aws_forensics"},{"location":"recipe-list/#aws_logging_collect","text":"Collects logs from an AWS account and dumps the results to the filesystem. Details: Collects logs from an AWS account using a specified query filter and date ranges, and dumps them on the filesystem. If no args are provided this recipe will collect 90 days of logs for the default AWS profile. CLI parameters: Parameter Default value Description region None AWS Region --profile_name None Name of the AWS profile to collect logs from. --query_filter None Filter expression to use to query logs. --start_time None Start time for the query. --end_time None End time for the query. Modules: AWSLogsCollector Module graph","title":"aws_logging_collect"},{"location":"recipe-list/#aws_logging_ts","text":"Collects logs from an AWS account, processes the logs with Plaso and uploads the result to Timesketch. Details: Collects logs from an AWS account using a specified query filter and date ranges, processes the logs with plaso and uploads the result to Timesketch. If no args are provided this recipe will collect 90 days of logs for the default AWS profile. CLI parameters: Parameter Default value Description region None AWS Region --profile_name None Name of the AWS profile to collect logs from. --query_filter None Filter expression to use to query logs. --start_time None Start time for the query. --end_time None End time for the query. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: AWSLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"aws_logging_ts"},{"location":"recipe-list/#aws_turbinia_ts","text":"Copies EBS volumes from within AWS, transfers them to GCP, analyses with Turbinia and exports the results to Timesketch. Details: Copies EBS volumes from within AWS, uses buckets and cloud-to-cloud operations to transfer the data to GCP. Once in GCP, a persistent disk is created and a job is added to the Turbinia queue to start analysis. The resulting Plaso file is then exported to Timesketch. CLI parameters: Parameter Default value Description aws_region None AWS region containing the EBS volumes. gcp_zone None Destination GCP zone in which to create the disks. volumes None Comma separated list of EBS volume IDs (e.g. vol-xxxxxxxx). aws_bucket None AWS bucket for image storage. gcp_bucket None GCP bucket for image storage. --subnet None AWS subnet to copy instances from, required if there is no default subnet in the volume region. --gcp_project None Destination GCP project. --aws_profile None Source AWS profile. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --request_ids None Comma separated Turbinia request identifiers to process. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --turbinia_zone 'us-central1-f' Zone Turbinia is located in --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. Modules: AWSVolumeSnapshotCollector , AWSSnapshotS3CopyCollector , S3ToGCSCopy , GCSToGCEImage , GCEDiskFromImage , TurbiniaGCPProcessor , TimesketchExporter Module graph","title":"aws_turbinia_ts"},{"location":"recipe-list/#azure_forensics","text":"Copies a disk from an Azure account to an analysis VM. Details: Copies a disk from an Azure account, creates an analysis VM in Azure (with a startup script containing installation instructions for basic forensics tooling), and attaches the copied disk to it. CLI parameters: Parameter Default value Description remote_profile_name None Name of the Azure profile pointing to the Azure account where the disk(s) exist(s). analysis_resource_group_name None The Azure resource group name in which to create the VM. incident_id None Incident ID to label the VM with. ssh_public_key None A SSH public key string to add to the VM (e.g. ssh-rsa AAAAB3NzaC1y... ). --instance_name None Instance name of the instance to analyze. --disk_names None Comma-separated list of disk names to copy. --all_disks False Copy all disks in the designated instance. Overrides disk_names if specified. --boot_disk_size '50' The size of the analysis VM's boot disk (in GB). --analysis_region None The Azure region in which to create the VM. --analysis_profile_name None Name of the Azure profile to use when creating the analysis VM. Modules: AzureCollector Module graph","title":"azure_forensics"},{"location":"recipe-list/#azure_logging_collect","text":"Collects logs from an Azure subscription and dumps the results to the filesystem. Details: Collects logs from an Azure subscription using a specified filter, and dumps them on the filesystem. CLI parameters: Parameter Default value Description subscription_id None Subscription ID for the subscription to collect logs from. filter_expression None A filter expression to use for the log query, must specify at least a start date like \"eventTimestamp ge '2022-02-01'\" --profile_name None A profile name to use when looking for Azure credentials. Modules: AzureLogsCollector Module graph","title":"azure_logging_collect"},{"location":"recipe-list/#azure_logging_ts","text":"Collects logs from an Azure subscription, processes the logs with Plaso and uploads the result to Timesketch. Details: Collects logs from an Azure subscription using a specified query filter and date ranges, processes the logs with plaso and uploads the result to Timesketch. CLI parameters: Parameter Default value Description subscription_id None Subscription ID for the subscription to collect logs from. filter_expression None A filter expression to use for the log query, must specify at least a start date like \"eventTimestamp ge '2022-02-01'\" --profile_name None A profile name to use when looking for Azure credentials. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: AzureLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"azure_logging_ts"},{"location":"recipe-list/#bigquery_collect","text":"Collects results from BigQuery and dumps them on the filesystem. Details: Collects results from BigQuery in a GCP project and dumps them in JSONL on the local filesystem. CLI parameters: Parameter Default value Description project_name None Name of GCP project to collect logs from. query None Query to execute. description None Human-readable description of the query. Modules: BigQueryCollector Module graph","title":"bigquery_collect"},{"location":"recipe-list/#bigquery_ts","text":"Collects results from BigQuery and uploads them to Timesketch. Details: Collects results from BigQuery in JSONL form, dumps them to the filesystem, and uploads them to Timesketch. CLI parameters: Parameter Default value Description project_name None Name of GCP project to collect logs from. query None Query to execute. description None Human-readable description of the query. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: BigQueryCollector , TimesketchExporter Module graph","title":"bigquery_ts"},{"location":"recipe-list/#gce_disk_copy","text":"Copy disks from one project to another. Details: Copies disks from one project to another. The disks can be specified individually, or instances can be specified, to copy all their disks or boot disks. CLI parameters: Parameter Default value Description source_project_name None Source project containing the disks to export. --destination_project_name None Project to where the disk images are exported. If not provided, source_project_name is used. --source_disk_names None Comma-separated list of disk names to export. If not provided, disks attached to remote_instance_name will be used. --remote_instance_names None Comma-separated list of instances in source project from which to copy disks. If not provided, disk_names will be used. --all_disks False If True, copy all disks attached to the remote_instance_names instances. If False and remote_instance_name is provided, it will select the instance's boot disk. --zone 'us-central1-f' Destination zone for the disks to be copied to. --stop_instances False Stop instances after disks have been copied Modules: GCEDiskCopy Module graph","title":"gce_disk_copy"},{"location":"recipe-list/#gce_disk_export","text":"Export a disk image from a GCP project to a Google Cloud Storage bucket. Details: Creates a disk image from Google Compute persistent disks, compresses the images, and exports them to Google Cloud Storage. The exported images names are appended by .tar.gz. As this export happens through a Cloud Build job, the default service account [PROJECT-NUMBER]@cloudbuild.gserviceaccount.com in the source or analysis project (if provided) must have the IAM role [Storage Admin] on their corresponding project's storage bucket/folder. CLI parameters: Parameter Default value Description source_project_name None Source project containing the disk to export. gcs_output_location None Google Cloud Storage parent bucket/folder to which to export the image. --analysis_project_name None Project where the disk image is created then exported. If not provided, the image is exported to a bucket in the source project. --source_disk_names None Comma-separated list of disk names to export. If not provided, disks attached to remote_instance_name will be used. --remote_instance_name None Instance in source project to export its disks. If not provided, disk_names will be used. --all_disks False If True, copy all disks attached to the remote_instance_name instance. If False and remote_instance_name is provided, it will select the instance's boot disk. --exported_image_name None Name of the output file, must comply with ^[A-Za-z0-9-]*$ and '.tar.gz' will be appended to the name. If not provided or if more than one disk is selected, the exported image will be named exported-image-{TIMESTAMP('YYYYmmddHHMMSS')} . --image_format '' An image format to use. Example values: qcow2, vmdk. Default (empty value) will be .tar.gz. Modules: GoogleCloudDiskExport Module graph","title":"gce_disk_export"},{"location":"recipe-list/#gce_disk_export_dd","text":"Stream the disk bytes from a GCP project to a Google Cloud Storage bucket. Details: The export is performed via bit streaming the the disk bytes to GCS. This will allow getting a disk image out of the project in case both organization policies constraints/compute.storageResourceUseRestrictions and constraints/compute.trustedImageProjects are enforced and in case OsLogin is allowed only for the organization users while the analyst is an external user with no roles/ compute.osLoginExternalUser role. The exported images names are appended by .tar.gz. The compute engine default service account in the source project must have sufficient permissions to Create and List Storage objects on the corresponding storage bucket/folder. CLI parameters: Parameter Default value Description source_project_name None Source project containing the disk to export. gcs_output_location None Google Cloud Storage parent bucket/folder to which to export the image. --source_disk_names None Comma-separated list of disk names to export. If not provided, disks attached to remote_instance_name will be used. --remote_instance_name None Instance in source project to export its disks. If not provided, source_disk_names will be used. --all_disks False If True, copy all disks attached to the remote_instance_name instance. If False and remote_instance_name is provided, it will select the instance's boot disk. --boot_image_project 'debian-cloud' Name of the project where the boot disk image of the export VM is stored. --boot_image_family 'debian-10' Name of the image to use to create the boot disk of the export VM. Modules: GoogleCloudDiskExportStream Module graph","title":"gce_disk_export_dd"},{"location":"recipe-list/#gcp_cloud_resource_tree","text":"Generates a parent/children tree for given GCP resource. Details: Generates a parent/children tree for given GCP resource by enumerating all the currently available resources. It also will attempt to fill any gaps identified in the tree through querying the GCP logs CLI parameters: Parameter Default value Description project_id None ID of the project where the resource is located location None Resource location (zone/region) or 'global' resource_type None Resource type (currently supported types: gce_instance, gce_disk, gce_image, gce_machine_image, gce_instance_template, gce_snapshot) --resource_id None Resource id --resource_name None Resource name Modules: GCPCloudResourceTree Module graph","title":"gcp_cloud_resource_tree"},{"location":"recipe-list/#gcp_cloud_resource_tree_offline","text":"Generates a parent/children tree for given GCP resource using the supplied exported GCP logs Details: Generates a parent/children tree for given GCP resource using the supplied exported GCP logs CLI parameters: Parameter Default value Description project_id None ID of the project where the resource is located location None Resource location (zone/region) or 'global' resource_type None Resource type (currently supported types: gce_instance, gce_disk, gce_image, gce_machine_image, gce_instance_template, gce_snapshot) paths None Comma-separated paths to GCP log files. Log files should contain log entiries in json format. --resource_id None Resource id --resource_name None Resource name Modules: FilesystemCollector , GCPCloudResourceTree Module graph","title":"gcp_cloud_resource_tree_offline"},{"location":"recipe-list/#gcp_forensics","text":"Copies disk from a GCP project to an analysis VM. Details: Copies a persistent disk from a GCP project to another, creates an analysis VM (with a startup script containing installation instructions for basic forensics tooling) in the destination project, and attaches the copied GCP persistent disk to it. CLI parameters: Parameter Default value Description source_project_name None Name of the project containing the instance / disks to copy. analysis_project_name None Name of the project where the analysis VM will be created and disks copied to. --incident_id None Incident ID to label the VM with. --instances None Name of the instance to analyze. --disks None Comma-separated list of disks to copy from the source GCP project (if instance not provided). --all_disks False Copy all disks in the designated instance. Overrides disk_names if specified. --stop_instances False Stop the designated instance after copying disks. --create_analysis_vm True Create an analysis VM in the destination project. --cpu_cores '4' Number of CPU cores of the analysis VM. --boot_disk_size '50' The size of the analysis VM boot disk (in GB). --boot_disk_type 'pd-standard' Disk type to use [pd-standard, pd-ssd]. --zone 'us-central1-f' The GCP zone where the Analysis VM and copied disks will be created. --analysis_vm_name 'gcp-forensics-vm' Name (prefix) to give the analysis vm. Modules: GCEDiskCopy , GCEForensicsVM Module graph","title":"gcp_forensics"},{"location":"recipe-list/#gcp_logging_cloudaudit_ts","text":"Collects GCP logs from a project and exports them to Timesketch. Details: Collects GCP logs from a project and exports them to Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. start_date None Start date. end_date None End date. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"gcp_logging_cloudaudit_ts"},{"location":"recipe-list/#gcp_logging_cloudsql_ts","text":"Collects GCP related to Cloud SQL instances in a project and exports them to Timesketch. Details: Collects GCP related to Cloud SQL instances in a project and exports them to Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. start_date None Start date. end_date None End date. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"gcp_logging_cloudsql_ts"},{"location":"recipe-list/#gcp_logging_collect","text":"Collects logs from a GCP project and dumps on the filesystem (JSON). https://cloud.google.com/logging/docs/view/query-library for example queries. Details: Collects logs from a GCP project and dumps on the filesystem. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. filter_expression \"resource.type = 'gce_instance'\" Filter expression to use to query GCP logs. See https://cloud.google.com/logging/docs/view/query-library for examples. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector Module graph","title":"gcp_logging_collect"},{"location":"recipe-list/#gcp_logging_gce_instance_ts","text":"GCP Instance Cloud Audit logs to Timesketch Details: Collects GCP Cloud Audit Logs for a GCE instance and exports them to Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. instance_id None Identifier for GCE instance (Instance ID). --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"gcp_logging_gce_instance_ts"},{"location":"recipe-list/#gcp_logging_gce_ts","text":"Loads all GCE Cloud Audit Logs in a GCP project into Timesketch. Details: Loads all GCE Cloud Audit Logs for all instances in a GCP project into Timesketch. Some light processing is made to translate the logs into something Timesketch can process. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. start_date None Start date. end_date None End date. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits Modules: GCPLogsCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"gcp_logging_gce_ts"},{"location":"recipe-list/#gcp_logging_ts","text":"Collects logs from a GCP project and sends them to Timesketch. Details: Collects logs from a GCP project and sends them to Timesketch. https://cloud.google.com/logging/docs/view/query-library for example queries. CLI parameters: Parameter Default value Description project_name None Name of the GCP project to collect logs from. filter_expression \"resource.type = 'gce_instance'\" Filter expression to use to query GCP logs. See https://cloud.google.com/logging/docs/view/query-library for examples. --backoff True If GCP Cloud Logging API query limits are exceeded, retry with an increased delay between each query to try complete the query at a slower rate. --delay '0' Number of seconds to wait between each GCP Cloud Logging query to avoid hitting API query limits --analyzers None Timesketch analyzers to run. --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --incident_id None Incident ID (used for Timesketch description). --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: GCPLogsCollector , GCPLoggingTimesketch , TimesketchExporter Module graph","title":"gcp_logging_ts"},{"location":"recipe-list/#gcp_turbinia_disk_copy_ts","text":"Imports a remote GCP persistent disk, processes it with Turbinia and sends results to Timesketch. Details: Imports a remote GCP persistent disk into an analysis GCP project and sends the result of Turbinia processing to Timesketch. Copies a disk from a remote GCP project into an analysis project Creates Turbinia processing request to process the imported disk Downloads and sends results of the Turbinia processing to Timesketch. This recipe will also start an analysis VM in the destination project with the attached disk (the same one that Turbinia will have processed). If the target disk is already in the same project as Turbinia, you can use the gcp_turbinia_ts recipe. CLI parameters: Parameter Default value Description source_project_name None Name of the project containing the instance / disks to copy. analysis_project_name None Name of the project containing the Turbinia instance. --request_ids None Comma separated Turbinia request identifiers to process. --yara_rules_path None Paths to Yara rules sent to Turbinia for processing. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --turbinia_zone 'us-central1-f' The GCP zone the disk to process and Turbinia workers are in. --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --incident_id None Incident ID (used for Timesketch description and to label the VM with). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --create_analysis_vm True Create an analysis VM in the destination project. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --instances None Name of the instances to analyze. --disks None Comma-separated list of disks to copy from the source GCP project (if instance not provided). --all_disks False Copy all disks in the designated instance. Overrides disk_names if specified. --stop_instances False Stop the designated instances after copying disks. --cpu_cores '4' Number of CPU cores of the analysis VM. --boot_disk_size '50' The size of the analysis VM boot disk (in GB). --boot_disk_type 'pd-standard' Disk type to use [pd-standard, pd-ssd] --image_project 'ubuntu-os-cloud' Name of the project where the analysis VM image is hosted. --image_family 'ubuntu-2204-lts' Name of the image to use to create the analysis VM. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. --analysis_vm_name 'gcp-forensics-vm' Name (prefix) to give the analysis vm. Modules: LocalYaraCollector , GCEDiskCopy , GCEForensicsVM , TurbiniaGCPProcessor , TimesketchExporter Module graph","title":"gcp_turbinia_disk_copy_ts"},{"location":"recipe-list/#gcp_turbinia_ts","text":"Processes existing GCP persistent disks with Turbinia project and sends results to Timesketch. Details: Process GCP persistent disks with Turbinia and send output to Timesketch. This processes disks that are already in the project where Turbinia exists. If you want to copy disks from another project, use the gcp_turbinia_disk_copy_ts recipe. CLI parameters: Parameter Default value Description analysis_project_name None Name of GCP project the disk exists in. turbinia_zone None The GCP zone the disk to process (and Turbinia workers) are in. --disk_names None Comma separated names of GCP persistent disks to process. This parameter can only be used if --request_ids is not provided. --request_ids None Comma separated Turbinia request identifiers to process. This parameter can only be used if --disk_names is not provided. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. Modules: TurbiniaGCPProcessor , TimesketchExporter Module graph","title":"gcp_turbinia_ts"},{"location":"recipe-list/#gdrive_collect","text":"Collect files from Google Drive. Details: Collect files from Google Drive. CLI parameters: Parameter Default value Description --folder_id None Parent Google drive folder ID to download from. Mutually exclusive with drive_ids. --recursive False Recursively collect from folder_id --drive_ids None Comma-separated list of drive IDs to download. Mutually exclusive with folder_id. --output_directory None Output directory for collected files. --overwrite_existing False Overwrite existing files. --max_download_workers 5 Maximum number of worker threads to use for downloading files. Modules: GoogleDriveCollector Module graph","title":"gdrive_collect"},{"location":"recipe-list/#grr_artifact_ts","text":"Fetches default ForensicArtifacts from a sequence of GRR hosts, processes them with plaso, and sends the results to Timesketch. Details: Collect artifacts from hosts using GRR. Collect a predefined list of artifacts from hosts using GRR Process them with a local install of plaso Export them to a Timesketch sketch. The default set of artifacts is defined in the GRRArtifactCollector module (see the _DEFAULT_ARTIFACTS_* class attributes in grr_hosts.py ), and varies per platform. CLI parameters: Parameter Default value Description hostnames None Comma-separated list of hostnames or GRR client IDs to process. reason None Reason for collection. --artifacts None Comma-separated list of artifacts to fetch (override default artifacts). --extra_artifacts None Comma-separated list of artifacts to append to the default artifact list. --use_raw_filesystem_access False Use raw disk access to fetch artifacts. --approvers None Emails for GRR approval request. --sketch_id None Timesketch sketch to which the timeline should be added. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --analyzers None Timesketch analyzers to run --token_password '' Optional custom password to decrypt Timesketch credential file with. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --incident_id None Incident ID (used for Timesketch description). --grr_server_url 'http://localhost:8000' GRR endpoint. --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --user_docker True Whether the LocalPlasoProcessor should use Docker or not. --max_file_size '5368709120' Maximum size of files to collect (in bytes). Modules: GRRArtifactCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"grr_artifact_ts"},{"location":"recipe-list/#grr_files_collect","text":"Collects specific files from one or more GRR hosts. Details: Collects specific files from one or more GRR hosts. Files can be a glob pattern (e.g. /tmp/*.so ) and support GRR variable interpolation (e.g. %%users.localappdata%%/Directory/ ) CLI parameters: Parameter Default value Description hostnames None Comma-separated list of hostnames or GRR client IDs to process. reason None Reason for collection. files None Comma-separated list of files to fetch (supports globs and GRR variable interpolation). directory None Directory in which to export files. --use_raw_filesystem_access False Use raw disk access to fetch artifacts. --approvers None Emails for GRR approval request. --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --action 'download' String denoting action (download/hash/stat) to take --grr_server_url 'http://localhost:8000' GRR endpoint --grr_username 'admin' GRR username --grr_password 'admin' GRR password --max_file_size '5368709120' Maximum size of files to collect (in bytes). Modules: GRRFileCollector , LocalFilesystemCopy Module graph","title":"grr_files_collect"},{"location":"recipe-list/#grr_flow_collect","text":"Download the result of a GRR flow to the local filesystem. Details: Download the result of a GRR flow to the local filesystem. Flow IDs are unique per client , so both need to be provided in sequence. CLI parameters: Parameter Default value Description hostnames None Hostname(s) to collect the flow from. flow_ids None Flow ID(s) to download. reason None Reason for collection. directory None Directory in which to export files. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'admin' GRR password Modules: GRRFlowCollector , LocalFilesystemCopy Module graph","title":"grr_flow_collect"},{"location":"recipe-list/#grr_hunt_artifacts","text":"Starts a GRR hunt for the default set of artifacts. Details: Starts a GRR artifact hunt and provides the Hunt ID to the user. Feed the Hunt ID to grr_huntresults_ts to process results through Plaso and export them to Timesketch. CLI parameters: Parameter Default value Description artifacts None Comma-separated list of artifacts to hunt for. reason None Reason for collection. --use_raw_filesystem_access False Use raw disk access to fetch artifacts. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --max_file_size '5368709120' Maximum size of files to collect (in bytes). --match_mode None Match mode of the client rule set (ANY or ALL) --client_operating_systems None Comma-separated list of client operating systems to filter hosts on (linux, osx, win). --client_labels None Comma-separated list of client labels to filter GRR hosts on. Modules: GRRHuntArtifactCollector Module graph","title":"grr_hunt_artifacts"},{"location":"recipe-list/#grr_hunt_file","text":"Starts a GRR hunt for a list of files. Details: Starts a GRR hunt for a list of files and provides a Hunt ID to the user. Feed the Hunt ID to grr_huntresults_ts to process results through Plaso and export them to Timesketch. Like in grr_files_collect , files can be globs and support variable interpolation. CLI parameters: Parameter Default value Description file_path_list None Comma-separated list of file paths to hunt for. reason None Reason for collection. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --max_file_size '5368709120' Maximum size of files to collect (in bytes). --match_mode None Match mode of the client rule set (ANY or ALL) --client_operating_systems None Comma-separated list of client operating systems to filter hosts on (linux, osx, win). --client_labels None Comma-separated list of client labels to filter GRR hosts on. Modules: GRRHuntFileCollector Module graph","title":"grr_hunt_file"},{"location":"recipe-list/#grr_hunt_osquery","text":"Starts a GRR hunt for an Osquery flow. Details: Starts a GRR osquery hunt and provides the Hunt ID to the user. CLI parameters: Parameter Default value Description reason None Reason for collection. --osquery_query None Osquery query to hunt for. --osquery_paths None Path(s) to text file containing one osquery query per line. --remote_configuration_path '' Path to a remote osquery configuration file on the GRR client. --local_configuration_path '' Path to a local osquery configuration file. --configuration_content '' Osquery configuration as a JSON string. --file_collection_columns '' The file collection columns. --timeout_millis '300000' Osquery timeout in milliseconds --ignore_stderr_errors False Ignore osquery stderr errors --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password --match_mode None Match mode of the client rule set (ANY or ALL) --client_operating_systems None Comma-separated list of client operating systems to filter hosts on (linux, osx, win). --client_labels None Comma-separated list of client labels to filter GRR hosts on. Modules: OsqueryCollector , GRRHuntOsqueryCollector Module graph","title":"grr_hunt_osquery"},{"location":"recipe-list/#grr_huntresults_ts","text":"Fetches the results of a GRR hunt, processes them with Plaso, and exports the results to Timesketch. Details: Download the results of a GRR hunt and process them. Collect results of a hunt given its Hunt ID Processes results with a local install of Plaso Exports processed items to a new Timesketch sketch CLI parameters: Parameter Default value Description hunt_id None ID of GRR Hunt results to fetch. reason None Reason for exporting hunt (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --grr_username 'admin' GRR username --grr_password 'admin' GRR password Modules: GRRHuntDownloader , LocalPlasoProcessor , TimesketchExporter Module graph","title":"grr_huntresults_ts"},{"location":"recipe-list/#grr_osquery_flow","text":"Runs osquery on GRR hosts and save any results to local CSV files. Details: Runs osquery on GRR hosts and save any results to local CSV files. CLI parameters: Parameter Default value Description reason None Reason for collection. hostnames None Hostname(s) to collect the osquery flow from. --osquery_query None Osquery query to hunt for. --osquery_paths None Path(s) to text file containing one osquery query per line. --remote_configuration_path '' Path to a remote osquery configuration file on the GRR client. --local_configuration_path '' Path to a local osquery configuration file. --configuration_content '' Osquery configuration as a JSON string. --file_collection_columns '' The file collection columns. --timeout_millis '300000' Osquery timeout in milliseconds --ignore_stderr_errors False Ignore osquery stderr errors --directory None Directory in which to export results. --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'admin' GRR password Modules: OsqueryCollector , GRROsqueryCollector Module graph","title":"grr_osquery_flow"},{"location":"recipe-list/#grr_timeline_ts","text":"Runs a TimelineFlow on a set of GRR hosts, generating a filesystem bodyfile for each host. These bodyfiles are processed results with Plaso, and the resulting plaso files are exported to Timesketch. Details: Uses the GRR TimelineFlow to generate a filesystem timeline and exports it to Timesketch.. CLI parameters: Parameter Default value Description hostnames None Comma-separated list of hostnames or GRR client IDs to process. root_path '/' Root path for timeline generation. reason None Reason for collection. --skip_offline_clients False Whether to skip clients that are offline. --approvers None Comma-separated list of usernames to ask for approval. --sketch_id None Timesketch sketch to which the timeline should be added. --grr_server_url 'http://localhost:8000' GRR endpoint. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --grr_username 'admin' GRR username. --grr_password 'admin' GRR password. --user_docker True Whether the LocalPlasoProcessor should use Docker or not. Modules: GRRTimelineCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"grr_timeline_ts"},{"location":"recipe-list/#grr_yarascan","text":"Run Yara rules on hosts memory. Details: Run Yara rules on hosts memory. CLI parameters: Parameter Default value Description reason None Reason for collection. hostnames None Hostname(s) to collect the flow from. --yara_name_filter None Filter to filter Yara sigs by. --dump_process_on_match False Whether to dump the process on match. --api_key None API Key to the Yeti instance --api_root 'http://localhost/api/' API root of the Yeti instance (e.g. http://localhost/api/) --approvers None Emails for GRR approval request. --grr_server_url 'http://localhost:8000' GRR endpoint --verify True Whether to verify the GRR TLS certificate. --skip_offline_clients False Whether to skip clients that are offline. --grr_username 'admin' GRR username --grr_password 'demo' GRR password Modules: YetiYaraCollector , GRRYaraScanner Module graph","title":"grr_yarascan"},{"location":"recipe-list/#gsheets_ts","text":"Collects data from google sheets and outputs them to Timesketch. Details: Collects data from google sheets and outputs them to Timesketch. CLI parameters: Parameter Default value Description spreadsheet None ID or URL of the Google Sheet spreadsheet to collect data from. --sheet_names [] Comma-separated list sheet names to collect date from. If not set all sheets in the spreadsheet will be parsed. --validate_columns True Set to True to check for mandatory columns required by Timesketch while extracting data. Set to False to ignore validation. Default is True. --sketch_id None Sketch to which the timeline should be added --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with --incident_id None Incident ID (used for Timesketch description) --wait_for_timelines True Whether to wait for timelines to finish processing. Modules: GoogleSheetsCollector , TimesketchExporter Module graph","title":"gsheets_ts"},{"location":"recipe-list/#openrelik_ts","text":"Processes files from the local file system using OpenRelik. Sends the results to Timesketch. Details: Processes files from the local file system using OpenRelik. Sends the results to Timesketch. CLI parameters: Parameter Default value Description paths None Comma-separated list of paths to process. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --folder_id None OpenRelik Folder ID. --template_workflow_id None OpenRelik workflow template ID. --openrelik_api 'http://localhost:8710' OpenRelik API server URI. --openrelik_ui 'http://localhost:8711' OpenRelik UI URI. --openrelik_api_key '' OpenRelik API key --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. --analyzers None Timesketch analyzers to run --token_password '' Optional custom password to decrypt Timesketch credential file with. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. Modules: FilesystemCollector , OpenRelikProcessor , TimesketchExporter Module graph","title":"openrelik_ts"},{"location":"recipe-list/#plaso_ts","text":"Processes a list of file paths using a Plaso and export results to Timesketch. Details: Processes a list of file paths using Plaso and sends results to Timesketch. Collectors collect from a path in the FS Processes them with a local install of plaso Exports them to a new Timesketch sketch CLI parameters: Parameter Default value Description paths None Comma-separated list of paths to process. --analyzers None Timesketch analyzers to run --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: FilesystemCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"plaso_ts"},{"location":"recipe-list/#ts_collect","text":"Collects Timesketch events. Details: Collects Timesketch events to a local file. CLI parameters: Parameter Default value Description directory None Directory in which to export files. query_string '*' The query string. Defaults to '*' (all events). --sketch_id None the Timesketch sketch ID. --start_datetime None The start datetime. --end_datetime None The end datetime. --indices None The comma-separated Timesketch indices. --labels None the comma-separated Timesketch event labels. --output_format 'csv' The output format (csv/json/jsonl). Defaults to csv --include_internal_columns False Include internal Timesketch fields in output. Defaults to false. --return_fields '*' The Timesketch fields to return from the search query. Defaults to *. --search_name None The search name (used as a filename prefix). --token_password '' Optional custom password to decrypt Timesketch credential file with. --timesketch_endpoint None The Timesketch endpoint URL --timesketch_username None The Timesketch username. --timesketch_password None The Timesketch password. Modules: TimesketchSearchEventCollector , LocalFilesystemCopy Module graph","title":"ts_collect"},{"location":"recipe-list/#upload_gdrive","text":"Uploads local file(s) to Google Drive. Details: Uploads file(s) to Google Drive. CLI parameters: Parameter Default value Description files None Comma-separated list of paths to files. --parent_folder_id '0B0m2ov3yrR0bNzBhNjcxNjctN2EyYS00ZTgxLTk3YjgtMTI5NGI1OGI3MTJh' --new_folder_name None Optional new folder name. --max_upload_workers 5 Maximum number of worker threads to use for uploading files. Modules: FilesystemCollector , GoogleDriveExporter Module graph","title":"upload_gdrive"},{"location":"recipe-list/#upload_ts","text":"Uploads a local CSV or Plaso file to Timesketch. Details: Uploads a CSV or Plaso file to Timesketch. CLI parameters: Parameter Default value Description files None Comma-separated list of paths to CSV files or Plaso storage files. --analyzers None Timesketch analyzers to run. --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --incident_id None Incident ID (used for Timesketch description). --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: FilesystemCollector , TimesketchExporter Module graph","title":"upload_ts"},{"location":"recipe-list/#upload_turbinia","text":"Uploads arbitrary files to Turbinia and downloads results. Details: Uploads arbitrary files to Turbinia for processing. The recipe will wait for Turbinia to return with results and will download them back to the filesystem. The Turbinia system needs to be accessible via SSH. CLI parameters: Parameter Default value Description files None Paths to process. --turbinia_recipe None The Turbinia recipe name to use for evidence processing. --destination_turbinia_dir None Destination path in Turbinia host to write the files to. --hostname None Remote host. --directory None Directory in which to copy and compress files. --turbinia_auth False Flag to indicate whether Turbinia API server requires authentication. --turbinia_api 'http://127.0.0.1:8000' Turbinia API server endpoint. --local_turbinia_results None Directory where Turbinia results will be downloaded to. --turbinia_zone 'us-central1-f' The GCP zone the disk to process and Turbinia workers are in. --sketch_id None Timesketch sketch ID. --priority_filter '100' Filter report findings, range from 0 to 100, 0 is the highest. Modules: FilesystemCollector , LocalFilesystemCopy , TurbiniaArtifactProcessor Module graph","title":"upload_turbinia"},{"location":"recipe-list/#upload_web_ts","text":"Uploads a CSV/JSONL or Plaso file to Timesketch and runs web-related Timesketch analyzers. Details: Uploads a CSV or Plaso file to Timesketch and runs a series of web-related analyzers on the uploaded data. The following analyzers will run on the processed timeline: browser_search,browser_timeframe,account_finder,phishy_domains,evtx_gap,login,win_crash,safebrowsing,chain . CLI parameters: Parameter Default value Description files None Comma-separated list of paths to CSV files or Plaso storage files. --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --incident_id None Incident ID (used for Timesketch description). --wait_for_analyzers True Wait for analyzers until they complete their run, if set to False the TS enhancer will be skipped. --searches_to_skip None A comma separated list of saved searches that should not be uploaded. --analyzer_max_checks '0' Number of wait cycles (per cycle is 3 seconds) before terminating wait for analyzers to complete. --aggregations_to_skip None A comma separated list of aggregation names that should not be uploaded. Modules: FilesystemCollector , TimesketchExporter Module graph","title":"upload_web_ts"},{"location":"recipe-list/#vt_evtx","text":"Downloads the EVTX files from VirusTotal for a specific hash. Details: Downloads the EVTX files from VirusTotal sandbox run for a specific hash, processes it with Plaso. CLI parameters: Parameter Default value Description hashes None Comma-separated list of hashes to process. directory None Directory in which to export files. --vt_api_key 'admin' Virustotal API key Modules: VTCollector , LocalPlasoProcessor Module graph","title":"vt_evtx"},{"location":"recipe-list/#vt_evtx_ts","text":"Downloads the EVTX from VirusTotal sandbox runs for a specific hash and uploads the corresponding timeline to Timesketch. Details: Downloads the EVTX file generated by VirusTotal during the sandbox runs for a specific hash, processes the EVTX files with Plaso and uploads the resulting Plaso file to Timesketch. CLI parameters: Parameter Default value Description hashes None Comma-separated list of hashes to process. directory None Directory in which to export files. --vt_api_key 'admin' Virustotal API key --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: VTCollector , LocalPlasoProcessor , TimesketchExporter Module graph","title":"vt_evtx_ts"},{"location":"recipe-list/#vt_pcap","text":"Downloads the PCAP from VirusTotal for a specific hash. Details: Downloads the PCAP files generated from VirusTotal sandbox's run for a specific hash. CLI parameters: Parameter Default value Description hashes None Comma-separated list of hashes to process. directory None Directory in which to export files. --vt_api_key 'admin' Virustotal API key Modules: VTCollector , LocalFilesystemCopy Module graph","title":"vt_pcap"},{"location":"recipe-list/#workspace_logging_collect","text":"Collects Workspace Audit logs and dumps them on the filesystem. Details: Collects logs from Workspace Audit log and dumps them on the filesystem. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list#ApplicationName for a list of application names. For filters, see https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list. CLI parameters: Parameter Default value Description application_name None Name of application to to collect logs for. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list#ApplicationName for a list of possible values. --user 'all' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list. Modules: WorkspaceAuditCollector Module graph","title":"workspace_logging_collect"},{"location":"recipe-list/#workspace_meet_ts","text":"Collects Meet records and adds them to Timesketch Details: Collects Google Workspace audit records or a Google Meet and adds them to Timesketch. CLI parameters: Parameter Default value Description meeting_id None ID for the Meeting to look up. (Without the '-' delimiter) --start_time None Start time. --end_time None End time. --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph","title":"workspace_meet_ts"},{"location":"recipe-list/#workspace_user_activity_ts","text":"Collects records for a Google Workspace user and adds them to Timesketch Details: Collects records for a Google Workspace user and adds them to Timesketch. Collects logs for the following apps: Login , Drive , Token , Chrome , CAA , DataStudio , GroupsEnterprise , Calendar , Chat , Groups , Meet , UserAccounts . CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time (yyyy-mm-ddTHH:MM:SSZ). --end_time None End time (yyyy-mm-ddTHH:MM:SSZ). --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector-Login , WorkspaceAuditCollector-Drive , WorkspaceAuditCollector-Token , WorkspaceAuditCollector-Chrome , WorkspaceAuditCollector-CAA , WorkspaceAuditCollector-DataStudio , WorkspaceAuditCollector-GroupsEnterprise , WorkspaceAuditCollector-Calendar , WorkspaceAuditCollector-Chat , WorkspaceAuditCollector-GCP , WorkspaceAuditCollector-Groups , WorkspaceAuditCollector-Meet , WorkspaceAuditCollector-UserAccounts , WorkspaceAuditTimesketch , TimesketchExporter Module graph","title":"workspace_user_activity_ts"},{"location":"recipe-list/#workspace_user_device_ts","text":"Collects mobile records and adds to Timesketch Details: Collects mobile (Device Audit activity) records for a Workspace user and adds them to Timesketch. CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph","title":"workspace_user_device_ts"},{"location":"recipe-list/#workspace_user_drive_ts","text":"Collects Drive records for a Workspace user and adds them to Timesketch Details: Collects Drive records for a Workspace user and adds them to Timesketch. CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph","title":"workspace_user_drive_ts"},{"location":"recipe-list/#workspace_user_login_ts","text":"Collects login records and adds to Timesketch Details: Collects login records for a Workspace user and adds them to Timesketch. CLI parameters: Parameter Default value Description user '' email address of the user to query logs for --start_time None Start time. --end_time None End time. --filter_expression '' Filter expression to use to query Workspace logs. See https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list --incident_id None Incident ID (used for Timesketch description). --sketch_id None Timesketch sketch to which the timeline should be added. --timesketch_endpoint 'http://localhost:5000/' Timesketch endpoint --timesketch_username None Username for Timesketch server. --timesketch_password None Password for Timesketch server. --token_password '' Optional custom password to decrypt Timesketch credential file with. --wait_for_timelines True Whether to wait for Timesketch to finish processing all timelines. Modules: WorkspaceAuditCollector , WorkspaceAuditTimesketch , TimesketchExporter Module graph","title":"workspace_user_login_ts"},{"location":"user-manual/","text":"User manual \u00b6 dfTimewolf ships with recipes , which are essentially instructions on how to launch and chain modules. Listing all recipes \u00b6 Since you won't know all the recipe names off the top of your head, start with: $ dftimewolf -h [2020-10-06 14:29:42,111] [dftimewolf ] INFO Logging to stdout and /tmp/dftimewolf.log [2020-10-06 14:29:42,111] [dftimewolf ] DEBUG Recipe data path: /Users/tomchop/code/dftimewolf/data [2020-10-06 14:29:42,112] [dftimewolf ] DEBUG Configuration loaded from: /Users/tomchop/code/dftimewolf/data/config.json usage: dftimewolf [-h] {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,gcp_logging_cloudsql_ts,gcp_logging_collect,gcp_logging_gce_instance_ts,gcp_logging_gce_ts,gcp_turbinia_disk_copy_ts,gcp_turbinia_ts,grr_artifact_grep,grr_artifact_ts,grr_files_collect,grr_flow_collect,grr_hunt_artifacts,grr_hunt_file,grr_huntresults_ts,plaso_ts,upload_ts,upload_turbinia,upload_web_ts,vt_pcap_ts} ... Available recipes: aws_forensics Copies a volume from an AWS account to an analysis VM. aws_logging_collect Collects logs from an AWS account and dumps on the filesystem. azure_forensics Copies a disk from an Azure account to an analysis VM. gce_disk_export Export disk image from a GCP project to Google Cloud Storage. gcp_forensics Copies disk from a GCP project to an analysis VM. gcp_logging_cloudaudit_ts Collects GCP logs from a project and exports them to Timesketch. gcp_logging_cloudsql_ts Collects GCP logs from Cloud SQL instances for a project and exports them to Timesketch. gcp_logging_collect Collects logs from a GCP project and dumps on the filesystem. gcp_logging_gce_instance_ts GCP Instance Cloud Audit to Timesketch gcp_logging_gce_ts Loads GCP Cloud Audit Logs for GCE into Timesketch gcp_turbinia_disk_copy_ts Imports a remote GCP persistent disk, processes it with Turbinia and sends results to Timesketch. gcp_turbinia_ts Processes an existing GCP persistent disk in the Turbinia project and sends results to Timesketch. gcp_turbinia_ts_threaded Processes existing GCP persistent disks in the Turbinia project and sends results to Timesketch. grr_artifact_grep Fetches ForensicArtifacts from GRR hosts and runs grep with a list of keywords on them. grr_artifact_ts Fetches default artifacts from a list of GRR hosts, processes them with plaso, and sends the results to Timesketch. grr_files_collect Fetches specific files from one or more GRR hosts. grr_flow_collect Download GRR flows. Download a GRR flow's results to the local filesystem. grr_hunt_artifacts Starts a GRR hunt for the default set of artifacts. grr_hunt_file Starts a GRR hunt for a list of files. grr_huntresults_ts Fetches the findings of a GRR hunt, processes them with plaso, and sends the results to Timesketch. grr_timeline_ts Runs a TimelineFlow on a set of GRR hosts, processes results with plaso, and sends the timeline to Timesketch plaso_ts Processes a list of file paths using plaso and sends results to Timesketch. upload_ts Uploads a CSV or Plaso file to Timesketch. upload_turbinia Uploads arbitrary files to Turbinia. upload_web_ts Uploads a CSV/JSONL or Plaso file to Timesketch. vt_evtx Fetches the EVTX from VirusTotal sandbox run for a specific hash. vt_evtx_ts Fetches the EVTX from VirusTotal sandbox run for a specific hash and upload it to Timesketch. vt_pcap Fetches the PCAP from VirusTotal sandbox run for a specific hash workspace_logging_collect Collects Workspace Audit logs and dumps them on the filesystem. workspace_meet_ts Collects Meet records and adds to Timesketch workspace_user_activity_ts Collects records and adds to Timesketch workspace_user_drive_ts Collects Drive records and adds to Timesketch workspace_user_login_ts Collects login records and adds to Timesketch positional arguments: {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,gcp_logging_cloudsql_ts,gcp_logging_collect,gcp_logging_gce_instance_ts,gcp_logging_gce_ts,gcp_turbinia_disk_copy_ts,gcp_turbinia_ts,grr_artifact_grep,grr_artifact_ts,grr_files_collect,grr_flow_collect,grr_hunt_artifacts,grr_hunt_file,grr_huntresults_ts,plaso_ts,upload_ts,upload_turbinia,upload_web_ts,vt_pcap_ts} optional arguments: -h, --help show this help message and exit Get detailed help for a specific recipe \u00b6 To get more details on a specific recipe: $ dftimewolf grr_artifact_hosts -h [2020-10-06 14:31:40,553] [dftimewolf ] INFO Logging to stdout and /tmp/dftimewolf.log [2020-10-06 14:31:40,553] [dftimewolf ] DEBUG Recipe data path: /Users/tomchop/code/dftimewolf/data [2020-10-06 14:31:40,553] [dftimewolf ] DEBUG Configuration loaded from: /Users/tomchop/code/dftimewolf/data/config.json usage: dftimewolf_recipes.py plaso_ts [-h] [--incident_id INCIDENT_ID] [--sketch_id SKETCH_ID] [--token_password TOKEN_PASSWORD] paths Processes a list of file paths using plaso and sends results to Timesketch. - Collectors collect from a path in the FS - Processes them with a local install of plaso - Exports them to a new Timesketch sketch positional arguments: paths Paths to process optional arguments: -h, --help show this help message and exit --incident_id INCIDENT_ID Incident ID (used for Timesketch description) (default: None) --sketch_id SKETCH_ID Sketch to which the timeline should be added (default: None) --token_password TOKEN_PASSWORD Optional custom password to decrypt Timesketch credential file with (default: ) Running a recipe \u00b6 One typically invokes dftimewolf with a recipe name and a few arguments. For example: $ dftimewolf <RECIPE_NAME> arg1 arg2 --optarg1 optvalue1 Given the help output above, you can then use the recipe like this: $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason If you only want to collect browser activity: $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason --artifact_list=BrowserHistory In the same way, if you want to specify one (or more) approver(s): $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason --artifact_list=BrowserHistory --approvers=admin $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason --artifact_list=BrowserHistory --approvers=admin,tomchop ~/.dftimewolfrc \u00b6 If you want to set recipe arguments to specific values without typing them in the command-line (e.g. your development Timesketch server, or your favorite set of GRR approvers), you can use a .dftimewolfrc file. Just create a ~/.dftimewolfrc file containing a JSON dump of parameters to replace: $ cat ~/.dftimewolfrc { \"approvers\": \"approver@greendale.xyz\", \"ts_endpoint\": \"http://timesketch.greendale.xyz/\" } This will set your ts_endpoint and approvers parameters for all subsequent dftimewolf runs. You can still override these settings for one-shot usages by manually specifying the argument in the command-line. Remove colorization \u00b6 dfTimewolf output will not be colorized if the environment variable DFTIMEWOLF_NO_RAINBOW is set.","title":"User manual"},{"location":"user-manual/#user-manual","text":"dfTimewolf ships with recipes , which are essentially instructions on how to launch and chain modules.","title":"User manual"},{"location":"user-manual/#listing-all-recipes","text":"Since you won't know all the recipe names off the top of your head, start with: $ dftimewolf -h [2020-10-06 14:29:42,111] [dftimewolf ] INFO Logging to stdout and /tmp/dftimewolf.log [2020-10-06 14:29:42,111] [dftimewolf ] DEBUG Recipe data path: /Users/tomchop/code/dftimewolf/data [2020-10-06 14:29:42,112] [dftimewolf ] DEBUG Configuration loaded from: /Users/tomchop/code/dftimewolf/data/config.json usage: dftimewolf [-h] {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,gcp_logging_cloudsql_ts,gcp_logging_collect,gcp_logging_gce_instance_ts,gcp_logging_gce_ts,gcp_turbinia_disk_copy_ts,gcp_turbinia_ts,grr_artifact_grep,grr_artifact_ts,grr_files_collect,grr_flow_collect,grr_hunt_artifacts,grr_hunt_file,grr_huntresults_ts,plaso_ts,upload_ts,upload_turbinia,upload_web_ts,vt_pcap_ts} ... Available recipes: aws_forensics Copies a volume from an AWS account to an analysis VM. aws_logging_collect Collects logs from an AWS account and dumps on the filesystem. azure_forensics Copies a disk from an Azure account to an analysis VM. gce_disk_export Export disk image from a GCP project to Google Cloud Storage. gcp_forensics Copies disk from a GCP project to an analysis VM. gcp_logging_cloudaudit_ts Collects GCP logs from a project and exports them to Timesketch. gcp_logging_cloudsql_ts Collects GCP logs from Cloud SQL instances for a project and exports them to Timesketch. gcp_logging_collect Collects logs from a GCP project and dumps on the filesystem. gcp_logging_gce_instance_ts GCP Instance Cloud Audit to Timesketch gcp_logging_gce_ts Loads GCP Cloud Audit Logs for GCE into Timesketch gcp_turbinia_disk_copy_ts Imports a remote GCP persistent disk, processes it with Turbinia and sends results to Timesketch. gcp_turbinia_ts Processes an existing GCP persistent disk in the Turbinia project and sends results to Timesketch. gcp_turbinia_ts_threaded Processes existing GCP persistent disks in the Turbinia project and sends results to Timesketch. grr_artifact_grep Fetches ForensicArtifacts from GRR hosts and runs grep with a list of keywords on them. grr_artifact_ts Fetches default artifacts from a list of GRR hosts, processes them with plaso, and sends the results to Timesketch. grr_files_collect Fetches specific files from one or more GRR hosts. grr_flow_collect Download GRR flows. Download a GRR flow's results to the local filesystem. grr_hunt_artifacts Starts a GRR hunt for the default set of artifacts. grr_hunt_file Starts a GRR hunt for a list of files. grr_huntresults_ts Fetches the findings of a GRR hunt, processes them with plaso, and sends the results to Timesketch. grr_timeline_ts Runs a TimelineFlow on a set of GRR hosts, processes results with plaso, and sends the timeline to Timesketch plaso_ts Processes a list of file paths using plaso and sends results to Timesketch. upload_ts Uploads a CSV or Plaso file to Timesketch. upload_turbinia Uploads arbitrary files to Turbinia. upload_web_ts Uploads a CSV/JSONL or Plaso file to Timesketch. vt_evtx Fetches the EVTX from VirusTotal sandbox run for a specific hash. vt_evtx_ts Fetches the EVTX from VirusTotal sandbox run for a specific hash and upload it to Timesketch. vt_pcap Fetches the PCAP from VirusTotal sandbox run for a specific hash workspace_logging_collect Collects Workspace Audit logs and dumps them on the filesystem. workspace_meet_ts Collects Meet records and adds to Timesketch workspace_user_activity_ts Collects records and adds to Timesketch workspace_user_drive_ts Collects Drive records and adds to Timesketch workspace_user_login_ts Collects login records and adds to Timesketch positional arguments: {aws_forensics,gce_disk_export,gcp_forensics,gcp_logging_cloudaudit_ts,gcp_logging_cloudsql_ts,gcp_logging_collect,gcp_logging_gce_instance_ts,gcp_logging_gce_ts,gcp_turbinia_disk_copy_ts,gcp_turbinia_ts,grr_artifact_grep,grr_artifact_ts,grr_files_collect,grr_flow_collect,grr_hunt_artifacts,grr_hunt_file,grr_huntresults_ts,plaso_ts,upload_ts,upload_turbinia,upload_web_ts,vt_pcap_ts} optional arguments: -h, --help show this help message and exit","title":"Listing all recipes"},{"location":"user-manual/#get-detailed-help-for-a-specific-recipe","text":"To get more details on a specific recipe: $ dftimewolf grr_artifact_hosts -h [2020-10-06 14:31:40,553] [dftimewolf ] INFO Logging to stdout and /tmp/dftimewolf.log [2020-10-06 14:31:40,553] [dftimewolf ] DEBUG Recipe data path: /Users/tomchop/code/dftimewolf/data [2020-10-06 14:31:40,553] [dftimewolf ] DEBUG Configuration loaded from: /Users/tomchop/code/dftimewolf/data/config.json usage: dftimewolf_recipes.py plaso_ts [-h] [--incident_id INCIDENT_ID] [--sketch_id SKETCH_ID] [--token_password TOKEN_PASSWORD] paths Processes a list of file paths using plaso and sends results to Timesketch. - Collectors collect from a path in the FS - Processes them with a local install of plaso - Exports them to a new Timesketch sketch positional arguments: paths Paths to process optional arguments: -h, --help show this help message and exit --incident_id INCIDENT_ID Incident ID (used for Timesketch description) (default: None) --sketch_id SKETCH_ID Sketch to which the timeline should be added (default: None) --token_password TOKEN_PASSWORD Optional custom password to decrypt Timesketch credential file with (default: )","title":"Get detailed help for a specific recipe"},{"location":"user-manual/#running-a-recipe","text":"One typically invokes dftimewolf with a recipe name and a few arguments. For example: $ dftimewolf <RECIPE_NAME> arg1 arg2 --optarg1 optvalue1 Given the help output above, you can then use the recipe like this: $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason If you only want to collect browser activity: $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason --artifact_list=BrowserHistory In the same way, if you want to specify one (or more) approver(s): $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason --artifact_list=BrowserHistory --approvers=admin $ dftimewolf grr_artifacts_ts tomchop.greendale.xyz collection_reason --artifact_list=BrowserHistory --approvers=admin,tomchop","title":"Running a recipe"},{"location":"user-manual/#dftimewolfrc","text":"If you want to set recipe arguments to specific values without typing them in the command-line (e.g. your development Timesketch server, or your favorite set of GRR approvers), you can use a .dftimewolfrc file. Just create a ~/.dftimewolfrc file containing a JSON dump of parameters to replace: $ cat ~/.dftimewolfrc { \"approvers\": \"approver@greendale.xyz\", \"ts_endpoint\": \"http://timesketch.greendale.xyz/\" } This will set your ts_endpoint and approvers parameters for all subsequent dftimewolf runs. You can still override these settings for one-shot usages by manually specifying the argument in the command-line.","title":"~/.dftimewolfrc"},{"location":"user-manual/#remove-colorization","text":"dfTimewolf output will not be colorized if the environment variable DFTIMEWOLF_NO_RAINBOW is set.","title":"Remove colorization"}]}